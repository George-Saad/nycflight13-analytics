18/06/29 02:51:31 INFO SparkContext: Running Spark version 2.2.0
18/06/29 02:51:32 INFO SparkContext: Submitted application: sparklyr
18/06/29 02:51:32 INFO SecurityManager: Changing view acls to: tehno-gate
18/06/29 02:51:32 INFO SecurityManager: Changing modify acls to: tehno-gate
18/06/29 02:51:32 INFO SecurityManager: Changing view acls groups to: 
18/06/29 02:51:32 INFO SecurityManager: Changing modify acls groups to: 
18/06/29 02:51:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tehno-gate); groups with view permissions: Set(); users  with modify permissions: Set(tehno-gate); groups with modify permissions: Set()
18/06/29 02:51:32 INFO Utils: Successfully started service 'sparkDriver' on port 54882.
18/06/29 02:51:33 INFO SparkEnv: Registering MapOutputTracker
18/06/29 02:51:33 INFO SparkEnv: Registering BlockManagerMaster
18/06/29 02:51:33 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
18/06/29 02:51:33 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
18/06/29 02:51:33 INFO DiskBlockManager: Created local directory at C:\Users\tehno-gate\AppData\Local\Temp\blockmgr-9ebb4fc7-0918-46dd-8b8a-14650211a170
18/06/29 02:51:33 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
18/06/29 02:51:33 INFO SparkEnv: Registering OutputCommitCoordinator
18/06/29 02:51:33 INFO Utils: Successfully started service 'SparkUI' on port 4040.
18/06/29 02:51:34 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://127.0.0.1:4040
18/06/29 02:51:34 INFO SparkContext: Added JAR file:/C:/Users/tehno-gate/Documents/R/win-library/3.4/sparklyr/java/sparklyr-2.2-2.11.jar at spark://127.0.0.1:54882/jars/sparklyr-2.2-2.11.jar with timestamp 1530229894101
18/06/29 02:51:34 INFO Executor: Starting executor ID driver on host localhost
18/06/29 02:51:34 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54903.
18/06/29 02:51:34 INFO NettyBlockTransferService: Server created on 127.0.0.1:54903
18/06/29 02:51:34 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
18/06/29 02:51:34 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 54903, None)
18/06/29 02:51:34 INFO BlockManagerMasterEndpoint: Registering block manager 127.0.0.1:54903 with 366.3 MB RAM, BlockManagerId(driver, 127.0.0.1, 54903, None)
18/06/29 02:51:34 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 54903, None)
18/06/29 02:51:34 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 127.0.0.1, 54903, None)
18/06/29 02:51:35 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.
18/06/29 02:51:35 INFO SharedState: loading hive config file: file:/C:/Users/tehno-gate/AppData/Local/spark/spark-2.2.0-bin-hadoop2.7/conf/hive-site.xml
18/06/29 02:51:35 INFO SharedState: Setting hive.metastore.warehouse.dir ('C:\Users\tehno-gate\AppData\Local\spark\spark-2.2.0-bin-hadoop2.7\tmp\hive') to the value of spark.sql.warehouse.dir ('C:Users	ehno-gateAppDataLocalsparkspark-2.2.0-bin-hadoop2.7	mphive').
18/06/29 02:51:35 INFO SharedState: Warehouse path is 'C:Users	ehno-gateAppDataLocalsparkspark-2.2.0-bin-hadoop2.7	mphive'.
18/06/29 02:51:36 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
18/06/29 02:51:37 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
18/06/29 02:51:38 INFO ObjectStore: ObjectStore, initialize called
18/06/29 02:51:38 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
18/06/29 02:51:38 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
18/06/29 02:51:40 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
18/06/29 02:51:42 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
18/06/29 02:51:42 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
18/06/29 02:51:43 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
18/06/29 02:51:43 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
18/06/29 02:51:43 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
18/06/29 02:51:43 INFO ObjectStore: Initialized ObjectStore
18/06/29 02:51:43 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
18/06/29 02:51:43 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
18/06/29 02:51:44 INFO HiveMetaStore: Added admin role in metastore
18/06/29 02:51:44 INFO HiveMetaStore: Added public role in metastore
18/06/29 02:51:44 INFO HiveMetaStore: No user is added in admin role, since config is empty
18/06/29 02:51:44 INFO HiveMetaStore: 0: get_all_databases
18/06/29 02:51:44 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_all_databases	
18/06/29 02:51:44 INFO HiveMetaStore: 0: get_functions: db=default pat=*
18/06/29 02:51:44 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
18/06/29 02:51:44 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
18/06/29 02:51:45 INFO SessionState: Created local directory: C:/Users/TEHNO-~1/AppData/Local/Temp/58dfe6df-9ad9-49be-ab93-4b51ec21c30a_resources
18/06/29 02:51:45 INFO SessionState: Created HDFS directory: C:/Users/tehno-gate/AppData/Local/spark/spark-2.2.0-bin-hadoop2.7/tmp/hive/tehno-gate/58dfe6df-9ad9-49be-ab93-4b51ec21c30a
18/06/29 02:51:45 INFO SessionState: Created local directory: C:/Users/tehno-gate/AppData/Local/spark/spark-2.2.0-bin-hadoop2.7/tmp/hive/58dfe6df-9ad9-49be-ab93-4b51ec21c30a
18/06/29 02:51:45 INFO SessionState: Created HDFS directory: C:/Users/tehno-gate/AppData/Local/spark/spark-2.2.0-bin-hadoop2.7/tmp/hive/tehno-gate/58dfe6df-9ad9-49be-ab93-4b51ec21c30a/_tmp_space.db
18/06/29 02:51:45 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is C:Users	ehno-gateAppDataLocalsparkspark-2.2.0-bin-hadoop2.7	mphive
18/06/29 02:51:45 INFO HiveMetaStore: 0: get_database: default
18/06/29 02:51:45 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: default	
18/06/29 02:51:45 INFO HiveMetaStore: 0: get_database: global_temp
18/06/29 02:51:45 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: global_temp	
18/06/29 02:51:45 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
18/06/29 02:51:46 INFO SessionState: Created local directory: C:/Users/TEHNO-~1/AppData/Local/Temp/f517f03d-72af-4338-9420-6a90f5446b50_resources
18/06/29 02:51:46 INFO SessionState: Created HDFS directory: C:/Users/tehno-gate/AppData/Local/spark/spark-2.2.0-bin-hadoop2.7/tmp/hive/tehno-gate/f517f03d-72af-4338-9420-6a90f5446b50
18/06/29 02:51:46 INFO SessionState: Created local directory: C:/Users/tehno-gate/AppData/Local/spark/spark-2.2.0-bin-hadoop2.7/tmp/hive/f517f03d-72af-4338-9420-6a90f5446b50
18/06/29 02:51:46 INFO SessionState: Created HDFS directory: C:/Users/tehno-gate/AppData/Local/spark/spark-2.2.0-bin-hadoop2.7/tmp/hive/tehno-gate/f517f03d-72af-4338-9420-6a90f5446b50/_tmp_space.db
18/06/29 02:51:46 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is C:Users	ehno-gateAppDataLocalsparkspark-2.2.0-bin-hadoop2.7	mphive
18/06/29 02:51:46 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
18/06/29 02:51:46 INFO SparkSqlParser: Parsing command: SHOW TABLES
18/06/29 02:51:48 INFO HiveMetaStore: 0: get_database: default
18/06/29 02:51:48 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: default	
18/06/29 02:51:48 INFO HiveMetaStore: 0: get_database: default
18/06/29 02:51:48 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: default	
18/06/29 02:51:48 INFO HiveMetaStore: 0: get_tables: db=default pat=*
18/06/29 02:51:48 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
18/06/29 02:51:49 INFO SparkContext: Starting job: collect at utils.scala:58
18/06/29 02:51:49 INFO DAGScheduler: Got job 0 (collect at utils.scala:58) with 1 output partitions
18/06/29 02:51:49 INFO DAGScheduler: Final stage: ResultStage 0 (collect at utils.scala:58)
18/06/29 02:51:49 INFO DAGScheduler: Parents of final stage: List()
18/06/29 02:51:49 INFO DAGScheduler: Missing parents: List()
18/06/29 02:51:49 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[4] at map at utils.scala:55), which has no missing parents
18/06/29 02:51:49 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 6.0 KB, free 366.3 MB)
18/06/29 02:51:50 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.4 KB, free 366.3 MB)
18/06/29 02:51:50 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 127.0.0.1:54903 (size: 3.4 KB, free: 366.3 MB)
18/06/29 02:51:50 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
18/06/29 02:51:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at map at utils.scala:55) (first 15 tasks are for partitions Vector(0))
18/06/29 02:51:50 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
18/06/29 02:51:50 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4852 bytes)
18/06/29 02:51:50 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
18/06/29 02:51:50 INFO Executor: Fetching spark://127.0.0.1:54882/jars/sparklyr-2.2-2.11.jar with timestamp 1530229894101
18/06/29 02:51:50 INFO TransportClientFactory: Successfully created connection to /127.0.0.1:54882 after 30 ms (0 ms spent in bootstraps)
18/06/29 02:51:50 INFO Utils: Fetching spark://127.0.0.1:54882/jars/sparklyr-2.2-2.11.jar to C:\Users\tehno-gate\AppData\Local\Temp\spark-78c33132-151d-4172-ac2d-6d050980bb7e\userFiles-a8778857-d57e-4271-9127-67156dce331b\fetchFileTemp6145981140457288987.tmp
18/06/29 02:51:50 INFO Executor: Adding file:/C:/Users/tehno-gate/AppData/Local/Temp/spark-78c33132-151d-4172-ac2d-6d050980bb7e/userFiles-a8778857-d57e-4271-9127-67156dce331b/sparklyr-2.2-2.11.jar to class loader
18/06/29 02:51:51 INFO CodeGenerator: Code generated in 380.208427 ms
18/06/29 02:51:51 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1013 bytes result sent to driver
18/06/29 02:51:51 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1144 ms on localhost (executor driver) (1/1)
18/06/29 02:51:51 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
18/06/29 02:51:51 INFO DAGScheduler: ResultStage 0 (collect at utils.scala:58) finished in 1.172 s
18/06/29 02:51:51 INFO DAGScheduler: Job 0 finished: collect at utils.scala:58, took 1.465712 s
18/06/29 02:52:07 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 02:52:07 INFO SparkSqlParser: Parsing command: flights
18/06/29 02:52:07 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 02:52:07 INFO SparkSqlParser: Parsing command: CACHE TABLE `flights`
18/06/29 02:52:07 INFO SparkSqlParser: Parsing command: `flights`
18/06/29 02:52:07 INFO CodeGenerator: Code generated in 26.611897 ms
18/06/29 02:52:07 INFO CodeGenerator: Code generated in 16.736879 ms
18/06/29 02:52:08 INFO SparkContext: Starting job: sql at NativeMethodAccessorImpl.java:0
18/06/29 02:52:08 INFO DAGScheduler: Registering RDD 12 (sql at NativeMethodAccessorImpl.java:0)
18/06/29 02:52:08 INFO DAGScheduler: Got job 1 (sql at NativeMethodAccessorImpl.java:0) with 1 output partitions
18/06/29 02:52:08 INFO DAGScheduler: Final stage: ResultStage 2 (sql at NativeMethodAccessorImpl.java:0)
18/06/29 02:52:08 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
18/06/29 02:52:08 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)
18/06/29 02:52:08 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[12] at sql at NativeMethodAccessorImpl.java:0), which has no missing parents
18/06/29 02:52:08 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 127.0.0.1:54903 in memory (size: 3.4 KB, free: 366.3 MB)
18/06/29 02:52:08 INFO ContextCleaner: Cleaned accumulator 51
18/06/29 02:52:08 INFO ContextCleaner: Cleaned accumulator 0
18/06/29 02:52:08 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 30.7 KB, free 366.3 MB)
18/06/29 02:52:08 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 11.8 KB, free 366.3 MB)
18/06/29 02:52:08 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 127.0.0.1:54903 (size: 11.8 KB, free: 366.3 MB)
18/06/29 02:52:08 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
18/06/29 02:52:08 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[12] at sql at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
18/06/29 02:52:08 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
18/06/29 02:52:08 WARN TaskSetManager: Stage 1 contains a task of very large size (27699 KB). The maximum recommended task size is 100 KB.
18/06/29 02:52:08 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 28364103 bytes)
18/06/29 02:52:08 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
18/06/29 02:52:09 INFO CodeGenerator: Code generated in 35.216089 ms
18/06/29 02:52:09 INFO CodeGenerator: Code generated in 237.42123 ms
18/06/29 02:52:15 INFO MemoryStore: Block rdd_9_0 stored as values in memory (estimated size 22.5 MB, free 343.8 MB)
18/06/29 02:52:15 INFO BlockManagerInfo: Added rdd_9_0 in memory on 127.0.0.1:54903 (size: 22.5 MB, free: 343.8 MB)
18/06/29 02:52:16 INFO CodeGenerator: Code generated in 7.937083 ms
18/06/29 02:52:16 INFO CodeGenerator: Code generated in 33.681437 ms
18/06/29 02:52:16 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2371 bytes result sent to driver
18/06/29 02:52:16 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 7838 ms on localhost (executor driver) (1/1)
18/06/29 02:52:16 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
18/06/29 02:52:16 INFO DAGScheduler: ShuffleMapStage 1 (sql at NativeMethodAccessorImpl.java:0) finished in 7.841 s
18/06/29 02:52:16 INFO DAGScheduler: looking for newly runnable stages
18/06/29 02:52:16 INFO DAGScheduler: running: Set()
18/06/29 02:52:16 INFO DAGScheduler: waiting: Set(ResultStage 2)
18/06/29 02:52:16 INFO DAGScheduler: failed: Set()
18/06/29 02:52:16 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[15] at sql at NativeMethodAccessorImpl.java:0), which has no missing parents
18/06/29 02:52:16 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 7.0 KB, free 343.8 MB)
18/06/29 02:52:16 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.7 KB, free 343.8 MB)
18/06/29 02:52:16 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 127.0.0.1:54903 (size: 3.7 KB, free: 343.8 MB)
18/06/29 02:52:16 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
18/06/29 02:52:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[15] at sql at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
18/06/29 02:52:16 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
18/06/29 02:52:16 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 4726 bytes)
18/06/29 02:52:16 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
18/06/29 02:52:16 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
18/06/29 02:52:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
18/06/29 02:52:16 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1581 bytes result sent to driver
18/06/29 02:52:16 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 60 ms on localhost (executor driver) (1/1)
18/06/29 02:52:16 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
18/06/29 02:52:16 INFO DAGScheduler: ResultStage 2 (sql at NativeMethodAccessorImpl.java:0) finished in 0.061 s
18/06/29 02:52:16 INFO DAGScheduler: Job 1 finished: sql at NativeMethodAccessorImpl.java:0, took 8.005903 s
18/06/29 02:52:16 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 02:52:16 INFO SparkSqlParser: Parsing command: SELECT count(*) FROM  `flights`
18/06/29 02:52:16 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 127.0.0.1:54903 in memory (size: 3.7 KB, free: 343.8 MB)
18/06/29 02:52:16 INFO HiveMetaStore: 0: get_database: default
18/06/29 02:52:16 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: default	
18/06/29 02:52:16 INFO SparkContext: Starting job: collect at utils.scala:211
18/06/29 02:52:16 INFO DAGScheduler: Registering RDD 18 (collect at utils.scala:211)
18/06/29 02:52:16 INFO DAGScheduler: Got job 2 (collect at utils.scala:211) with 1 output partitions
18/06/29 02:52:16 INFO DAGScheduler: Final stage: ResultStage 4 (collect at utils.scala:211)
18/06/29 02:52:16 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
18/06/29 02:52:16 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 3)
18/06/29 02:52:16 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[18] at collect at utils.scala:211), which has no missing parents
18/06/29 02:52:16 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 30.7 KB, free 343.7 MB)
18/06/29 02:52:16 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 11.7 KB, free 343.7 MB)
18/06/29 02:52:16 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 127.0.0.1:54903 (size: 11.7 KB, free: 343.8 MB)
18/06/29 02:52:16 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1006
18/06/29 02:52:16 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[18] at collect at utils.scala:211) (first 15 tasks are for partitions Vector(0))
18/06/29 02:52:16 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
18/06/29 02:52:16 WARN TaskSetManager: Stage 3 contains a task of very large size (27699 KB). The maximum recommended task size is 100 KB.
18/06/29 02:52:16 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 28364103 bytes)
18/06/29 02:52:16 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
18/06/29 02:52:17 INFO ContextCleaner: Cleaned accumulator 112
18/06/29 02:52:17 INFO BlockManager: Found block rdd_9_0 locally
18/06/29 02:52:17 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1733 bytes result sent to driver
18/06/29 02:52:17 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 812 ms on localhost (executor driver) (1/1)
18/06/29 02:52:17 INFO DAGScheduler: ShuffleMapStage 3 (collect at utils.scala:211) finished in 0.812 s
18/06/29 02:52:17 INFO DAGScheduler: looking for newly runnable stages
18/06/29 02:52:17 INFO DAGScheduler: running: Set()
18/06/29 02:52:17 INFO DAGScheduler: waiting: Set(ResultStage 4)
18/06/29 02:52:17 INFO DAGScheduler: failed: Set()
18/06/29 02:52:17 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[21] at collect at utils.scala:211), which has no missing parents
18/06/29 02:52:17 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
18/06/29 02:52:17 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 7.0 KB, free 343.7 MB)
18/06/29 02:52:17 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.7 KB, free 343.7 MB)
18/06/29 02:52:17 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 127.0.0.1:54903 (size: 3.7 KB, free: 343.8 MB)
18/06/29 02:52:17 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1006
18/06/29 02:52:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[21] at collect at utils.scala:211) (first 15 tasks are for partitions Vector(0))
18/06/29 02:52:17 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
18/06/29 02:52:17 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, ANY, 4726 bytes)
18/06/29 02:52:17 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
18/06/29 02:52:17 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
18/06/29 02:52:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
18/06/29 02:52:17 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1495 bytes result sent to driver
18/06/29 02:52:17 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 16 ms on localhost (executor driver) (1/1)
18/06/29 02:52:17 INFO DAGScheduler: ResultStage 4 (collect at utils.scala:211) finished in 0.018 s
18/06/29 02:52:17 INFO DAGScheduler: Job 2 finished: collect at utils.scala:211, took 0.868173 s
18/06/29 02:52:17 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
18/06/29 02:52:17 INFO CodeGenerator: Code generated in 11.870306 ms
18/06/29 02:52:17 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 02:52:17 INFO SparkSqlParser: Parsing command: SELECT *
FROM `flights` AS `zzz1`
WHERE (0 = 1)
18/06/29 02:52:18 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 02:52:18 INFO SparkSqlParser: Parsing command: SHOW TABLES
18/06/29 02:52:18 INFO HiveMetaStore: 0: get_database: default
18/06/29 02:52:18 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: default	
18/06/29 02:52:18 INFO HiveMetaStore: 0: get_database: default
18/06/29 02:52:18 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: default	
18/06/29 02:52:18 INFO HiveMetaStore: 0: get_tables: db=default pat=*
18/06/29 02:52:18 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
18/06/29 02:52:18 INFO CodeGenerator: Code generated in 13.206939 ms
18/06/29 02:52:18 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 02:52:18 INFO SparkSqlParser: Parsing command: SHOW TABLES
18/06/29 02:52:18 INFO HiveMetaStore: 0: get_database: default
18/06/29 02:52:18 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: default	
18/06/29 02:52:18 INFO HiveMetaStore: 0: get_database: default
18/06/29 02:52:18 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: default	
18/06/29 02:52:18 INFO HiveMetaStore: 0: get_tables: db=default pat=*
18/06/29 02:52:18 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
18/06/29 02:52:18 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 02:52:19 INFO SparkSqlParser: Parsing command: SHOW TABLES
18/06/29 02:52:19 INFO HiveMetaStore: 0: get_database: default
18/06/29 02:52:19 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: default	
18/06/29 02:52:19 INFO HiveMetaStore: 0: get_database: default
18/06/29 02:52:19 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: default	
18/06/29 02:52:19 INFO HiveMetaStore: 0: get_tables: db=default pat=*
18/06/29 02:52:19 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
18/06/29 02:55:08 INFO SparkContext: Running Spark version 2.2.0
18/06/29 02:55:09 INFO SparkContext: Submitted application: sparklyr
18/06/29 02:55:09 INFO SecurityManager: Changing view acls to: tehno-gate
18/06/29 02:55:09 INFO SecurityManager: Changing modify acls to: tehno-gate
18/06/29 02:55:09 INFO SecurityManager: Changing view acls groups to: 
18/06/29 02:55:09 INFO SecurityManager: Changing modify acls groups to: 
18/06/29 02:55:09 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tehno-gate); groups with view permissions: Set(); users  with modify permissions: Set(tehno-gate); groups with modify permissions: Set()
18/06/29 02:55:09 INFO Utils: Successfully started service 'sparkDriver' on port 54958.
18/06/29 02:55:09 INFO SparkEnv: Registering MapOutputTracker
18/06/29 02:55:09 INFO SparkEnv: Registering BlockManagerMaster
18/06/29 02:55:09 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
18/06/29 02:55:09 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
18/06/29 02:55:10 INFO DiskBlockManager: Created local directory at C:\Users\tehno-gate\AppData\Local\Temp\blockmgr-e31b5231-c0ab-450b-a81d-a3e6c872efc7
18/06/29 02:55:10 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
18/06/29 02:55:10 INFO SparkEnv: Registering OutputCommitCoordinator
18/06/29 02:55:10 INFO Utils: Successfully started service 'SparkUI' on port 4040.
18/06/29 02:55:10 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://127.0.0.1:4040
18/06/29 02:55:10 INFO SparkContext: Added JAR file:/C:/Users/tehno-gate/Documents/R/win-library/3.4/sparklyr/java/sparklyr-2.2-2.11.jar at spark://127.0.0.1:54958/jars/sparklyr-2.2-2.11.jar with timestamp 1530230110869
18/06/29 02:55:11 INFO Executor: Starting executor ID driver on host localhost
18/06/29 02:55:11 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54979.
18/06/29 02:55:11 INFO NettyBlockTransferService: Server created on 127.0.0.1:54979
18/06/29 02:55:11 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
18/06/29 02:55:11 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 54979, None)
18/06/29 02:55:11 INFO BlockManagerMasterEndpoint: Registering block manager 127.0.0.1:54979 with 366.3 MB RAM, BlockManagerId(driver, 127.0.0.1, 54979, None)
18/06/29 02:55:11 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 54979, None)
18/06/29 02:55:11 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 127.0.0.1, 54979, None)
18/06/29 02:55:12 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.
18/06/29 02:55:12 INFO SharedState: loading hive config file: file:/C:/Users/tehno-gate/AppData/Local/spark/spark-2.2.0-bin-hadoop2.7/conf/hive-site.xml
18/06/29 02:55:12 INFO SharedState: Setting hive.metastore.warehouse.dir ('C:\Users\tehno-gate\AppData\Local\spark\spark-2.2.0-bin-hadoop2.7\tmp\hive') to the value of spark.sql.warehouse.dir ('C:Users	ehno-gateAppDataLocalsparkspark-2.2.0-bin-hadoop2.7	mphive').
18/06/29 02:55:12 INFO SharedState: Warehouse path is 'C:Users	ehno-gateAppDataLocalsparkspark-2.2.0-bin-hadoop2.7	mphive'.
18/06/29 02:55:14 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
18/06/29 02:55:15 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
18/06/29 02:55:15 INFO ObjectStore: ObjectStore, initialize called
18/06/29 02:55:15 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
18/06/29 02:55:15 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
18/06/29 02:55:17 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
18/06/29 02:55:19 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
18/06/29 02:55:19 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
18/06/29 02:55:20 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
18/06/29 02:55:20 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
18/06/29 02:55:20 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
18/06/29 02:55:20 INFO ObjectStore: Initialized ObjectStore
18/06/29 02:55:21 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
18/06/29 02:55:21 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
18/06/29 02:55:21 INFO HiveMetaStore: Added admin role in metastore
18/06/29 02:55:21 INFO HiveMetaStore: Added public role in metastore
18/06/29 02:55:21 INFO HiveMetaStore: No user is added in admin role, since config is empty
18/06/29 02:55:21 INFO HiveMetaStore: 0: get_all_databases
18/06/29 02:55:21 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_all_databases	
18/06/29 02:55:21 INFO HiveMetaStore: 0: get_functions: db=default pat=*
18/06/29 02:55:21 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
18/06/29 02:55:21 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
18/06/29 02:55:22 INFO SessionState: Created local directory: C:/Users/TEHNO-~1/AppData/Local/Temp/61a89593-58d3-4f3f-992d-95647ab8d21f_resources
18/06/29 02:55:22 INFO SessionState: Created HDFS directory: C:/Users/tehno-gate/AppData/Local/spark/spark-2.2.0-bin-hadoop2.7/tmp/hive/tehno-gate/61a89593-58d3-4f3f-992d-95647ab8d21f
18/06/29 02:55:22 INFO SessionState: Created local directory: C:/Users/tehno-gate/AppData/Local/spark/spark-2.2.0-bin-hadoop2.7/tmp/hive/61a89593-58d3-4f3f-992d-95647ab8d21f
18/06/29 02:55:22 INFO SessionState: Created HDFS directory: C:/Users/tehno-gate/AppData/Local/spark/spark-2.2.0-bin-hadoop2.7/tmp/hive/tehno-gate/61a89593-58d3-4f3f-992d-95647ab8d21f/_tmp_space.db
18/06/29 02:55:22 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is C:Users	ehno-gateAppDataLocalsparkspark-2.2.0-bin-hadoop2.7	mphive
18/06/29 02:55:22 INFO HiveMetaStore: 0: get_database: default
18/06/29 02:55:22 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: default	
18/06/29 02:55:22 INFO HiveMetaStore: 0: get_database: global_temp
18/06/29 02:55:22 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: global_temp	
18/06/29 02:55:22 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
18/06/29 02:55:22 INFO SessionState: Created local directory: C:/Users/TEHNO-~1/AppData/Local/Temp/7913f14f-c968-4ed3-a6a4-313c2bd3c31c_resources
18/06/29 02:55:22 INFO SessionState: Created HDFS directory: C:/Users/tehno-gate/AppData/Local/spark/spark-2.2.0-bin-hadoop2.7/tmp/hive/tehno-gate/7913f14f-c968-4ed3-a6a4-313c2bd3c31c
18/06/29 02:55:22 INFO SessionState: Created local directory: C:/Users/tehno-gate/AppData/Local/spark/spark-2.2.0-bin-hadoop2.7/tmp/hive/7913f14f-c968-4ed3-a6a4-313c2bd3c31c
18/06/29 02:55:22 INFO SessionState: Created HDFS directory: C:/Users/tehno-gate/AppData/Local/spark/spark-2.2.0-bin-hadoop2.7/tmp/hive/tehno-gate/7913f14f-c968-4ed3-a6a4-313c2bd3c31c/_tmp_space.db
18/06/29 02:55:22 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is C:Users	ehno-gateAppDataLocalsparkspark-2.2.0-bin-hadoop2.7	mphive
18/06/29 02:55:22 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
18/06/29 02:55:22 INFO SparkSqlParser: Parsing command: SHOW TABLES
18/06/29 02:55:25 INFO HiveMetaStore: 0: get_database: default
18/06/29 02:55:25 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: default	
18/06/29 02:55:25 INFO HiveMetaStore: 0: get_database: default
18/06/29 02:55:25 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: default	
18/06/29 02:55:25 INFO HiveMetaStore: 0: get_tables: db=default pat=*
18/06/29 02:55:25 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
18/06/29 02:55:26 INFO SparkContext: Starting job: collect at utils.scala:58
18/06/29 02:55:26 INFO DAGScheduler: Got job 0 (collect at utils.scala:58) with 1 output partitions
18/06/29 02:55:26 INFO DAGScheduler: Final stage: ResultStage 0 (collect at utils.scala:58)
18/06/29 02:55:26 INFO DAGScheduler: Parents of final stage: List()
18/06/29 02:55:26 INFO DAGScheduler: Missing parents: List()
18/06/29 02:55:26 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[4] at map at utils.scala:55), which has no missing parents
18/06/29 02:55:26 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 6.0 KB, free 366.3 MB)
18/06/29 02:55:26 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.4 KB, free 366.3 MB)
18/06/29 02:55:26 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 127.0.0.1:54979 (size: 3.4 KB, free: 366.3 MB)
18/06/29 02:55:26 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
18/06/29 02:55:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at map at utils.scala:55) (first 15 tasks are for partitions Vector(0))
18/06/29 02:55:26 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
18/06/29 02:55:26 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4852 bytes)
18/06/29 02:55:26 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
18/06/29 02:55:26 INFO Executor: Fetching spark://127.0.0.1:54958/jars/sparklyr-2.2-2.11.jar with timestamp 1530230110869
18/06/29 02:55:26 INFO TransportClientFactory: Successfully created connection to /127.0.0.1:54958 after 32 ms (0 ms spent in bootstraps)
18/06/29 02:55:26 INFO Utils: Fetching spark://127.0.0.1:54958/jars/sparklyr-2.2-2.11.jar to C:\Users\tehno-gate\AppData\Local\Temp\spark-f81911d3-1c58-41ee-b3e4-5415430e7d20\userFiles-a1167d21-393b-4e3d-b012-1164e8570c43\fetchFileTemp3765844826963202002.tmp
18/06/29 02:55:27 INFO Executor: Adding file:/C:/Users/tehno-gate/AppData/Local/Temp/spark-f81911d3-1c58-41ee-b3e4-5415430e7d20/userFiles-a1167d21-393b-4e3d-b012-1164e8570c43/sparklyr-2.2-2.11.jar to class loader
18/06/29 02:55:27 INFO CodeGenerator: Code generated in 397.561099 ms
18/06/29 02:55:28 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 970 bytes result sent to driver
18/06/29 02:55:28 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1217 ms on localhost (executor driver) (1/1)
18/06/29 02:55:28 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
18/06/29 02:55:28 INFO DAGScheduler: ResultStage 0 (collect at utils.scala:58) finished in 1.254 s
18/06/29 02:55:28 INFO DAGScheduler: Job 0 finished: collect at utils.scala:58, took 1.607727 s
18/06/29 02:55:45 INFO ContextCleaner: Cleaned accumulator 0
18/06/29 02:55:45 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 127.0.0.1:54979 in memory (size: 3.4 KB, free: 366.3 MB)
18/06/29 02:55:45 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 02:55:45 INFO SparkSqlParser: Parsing command: flights
18/06/29 02:55:45 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 02:55:45 INFO SparkSqlParser: Parsing command: CACHE TABLE `flights`
18/06/29 02:55:45 INFO SparkSqlParser: Parsing command: `flights`
18/06/29 02:55:45 INFO CodeGenerator: Code generated in 25.708131 ms
18/06/29 02:55:45 INFO CodeGenerator: Code generated in 16.767065 ms
18/06/29 02:55:46 INFO SparkContext: Starting job: sql at NativeMethodAccessorImpl.java:0
18/06/29 02:55:46 INFO DAGScheduler: Registering RDD 12 (sql at NativeMethodAccessorImpl.java:0)
18/06/29 02:55:46 INFO DAGScheduler: Got job 1 (sql at NativeMethodAccessorImpl.java:0) with 1 output partitions
18/06/29 02:55:46 INFO DAGScheduler: Final stage: ResultStage 2 (sql at NativeMethodAccessorImpl.java:0)
18/06/29 02:55:46 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
18/06/29 02:55:46 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)
18/06/29 02:55:46 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[12] at sql at NativeMethodAccessorImpl.java:0), which has no missing parents
18/06/29 02:55:46 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 30.7 KB, free 366.3 MB)
18/06/29 02:55:46 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 11.8 KB, free 366.3 MB)
18/06/29 02:55:46 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 127.0.0.1:54979 (size: 11.8 KB, free: 366.3 MB)
18/06/29 02:55:46 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
18/06/29 02:55:46 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[12] at sql at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
18/06/29 02:55:46 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
18/06/29 02:55:46 WARN TaskSetManager: Stage 1 contains a task of very large size (27699 KB). The maximum recommended task size is 100 KB.
18/06/29 02:55:46 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 28364103 bytes)
18/06/29 02:55:46 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
18/06/29 02:55:47 INFO ContextCleaner: Cleaned accumulator 51
18/06/29 02:55:47 INFO CodeGenerator: Code generated in 57.18479 ms
18/06/29 02:55:47 INFO CodeGenerator: Code generated in 207.244376 ms
18/06/29 02:55:53 INFO MemoryStore: Block rdd_9_0 stored as values in memory (estimated size 22.5 MB, free 343.8 MB)
18/06/29 02:55:53 INFO BlockManagerInfo: Added rdd_9_0 in memory on 127.0.0.1:54979 (size: 22.5 MB, free: 343.8 MB)
18/06/29 02:55:53 INFO CodeGenerator: Code generated in 11.081247 ms
18/06/29 02:55:53 INFO CodeGenerator: Code generated in 28.644012 ms
18/06/29 02:55:53 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2371 bytes result sent to driver
18/06/29 02:55:53 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 7558 ms on localhost (executor driver) (1/1)
18/06/29 02:55:53 INFO DAGScheduler: ShuffleMapStage 1 (sql at NativeMethodAccessorImpl.java:0) finished in 7.560 s
18/06/29 02:55:53 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
18/06/29 02:55:53 INFO DAGScheduler: looking for newly runnable stages
18/06/29 02:55:53 INFO DAGScheduler: running: Set()
18/06/29 02:55:53 INFO DAGScheduler: waiting: Set(ResultStage 2)
18/06/29 02:55:53 INFO DAGScheduler: failed: Set()
18/06/29 02:55:53 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[15] at sql at NativeMethodAccessorImpl.java:0), which has no missing parents
18/06/29 02:55:53 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 7.0 KB, free 343.8 MB)
18/06/29 02:55:53 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.7 KB, free 343.8 MB)
18/06/29 02:55:53 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 127.0.0.1:54979 (size: 3.7 KB, free: 343.8 MB)
18/06/29 02:55:53 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
18/06/29 02:55:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[15] at sql at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
18/06/29 02:55:53 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
18/06/29 02:55:53 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 4726 bytes)
18/06/29 02:55:53 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
18/06/29 02:55:53 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
18/06/29 02:55:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
18/06/29 02:55:53 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1538 bytes result sent to driver
18/06/29 02:55:53 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 73 ms on localhost (executor driver) (1/1)
18/06/29 02:55:53 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
18/06/29 02:55:53 INFO DAGScheduler: ResultStage 2 (sql at NativeMethodAccessorImpl.java:0) finished in 0.074 s
18/06/29 02:55:53 INFO DAGScheduler: Job 1 finished: sql at NativeMethodAccessorImpl.java:0, took 7.729640 s
18/06/29 02:55:53 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 02:55:53 INFO SparkSqlParser: Parsing command: SELECT count(*) FROM  `flights`
18/06/29 02:55:53 INFO HiveMetaStore: 0: get_database: default
18/06/29 02:55:53 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: default	
18/06/29 02:55:53 INFO SparkContext: Starting job: collect at utils.scala:211
18/06/29 02:55:53 INFO DAGScheduler: Registering RDD 18 (collect at utils.scala:211)
18/06/29 02:55:53 INFO DAGScheduler: Got job 2 (collect at utils.scala:211) with 1 output partitions
18/06/29 02:55:53 INFO DAGScheduler: Final stage: ResultStage 4 (collect at utils.scala:211)
18/06/29 02:55:53 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
18/06/29 02:55:53 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 3)
18/06/29 02:55:53 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[18] at collect at utils.scala:211), which has no missing parents
18/06/29 02:55:53 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 30.7 KB, free 343.7 MB)
18/06/29 02:55:53 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 11.7 KB, free 343.7 MB)
18/06/29 02:55:53 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 127.0.0.1:54979 (size: 11.7 KB, free: 343.8 MB)
18/06/29 02:55:53 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1006
18/06/29 02:55:53 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[18] at collect at utils.scala:211) (first 15 tasks are for partitions Vector(0))
18/06/29 02:55:53 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
18/06/29 02:55:54 INFO ContextCleaner: Cleaned accumulator 112
18/06/29 02:55:54 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 127.0.0.1:54979 in memory (size: 3.7 KB, free: 343.8 MB)
18/06/29 02:55:54 WARN TaskSetManager: Stage 3 contains a task of very large size (27699 KB). The maximum recommended task size is 100 KB.
18/06/29 02:55:54 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 28364103 bytes)
18/06/29 02:55:54 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
18/06/29 02:55:54 INFO BlockManager: Found block rdd_9_0 locally
18/06/29 02:55:54 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1690 bytes result sent to driver
18/06/29 02:55:54 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 763 ms on localhost (executor driver) (1/1)
18/06/29 02:55:54 INFO DAGScheduler: ShuffleMapStage 3 (collect at utils.scala:211) finished in 0.764 s
18/06/29 02:55:54 INFO DAGScheduler: looking for newly runnable stages
18/06/29 02:55:54 INFO DAGScheduler: running: Set()
18/06/29 02:55:54 INFO DAGScheduler: waiting: Set(ResultStage 4)
18/06/29 02:55:54 INFO DAGScheduler: failed: Set()
18/06/29 02:55:54 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[21] at collect at utils.scala:211), which has no missing parents
18/06/29 02:55:54 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 7.0 KB, free 343.7 MB)
18/06/29 02:55:54 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.7 KB, free 343.7 MB)
18/06/29 02:55:54 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 127.0.0.1:54979 (size: 3.7 KB, free: 343.8 MB)
18/06/29 02:55:54 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
18/06/29 02:55:54 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1006
18/06/29 02:55:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[21] at collect at utils.scala:211) (first 15 tasks are for partitions Vector(0))
18/06/29 02:55:54 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
18/06/29 02:55:54 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, ANY, 4726 bytes)
18/06/29 02:55:54 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
18/06/29 02:55:54 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
18/06/29 02:55:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
18/06/29 02:55:54 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1495 bytes result sent to driver
18/06/29 02:55:54 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 9 ms on localhost (executor driver) (1/1)
18/06/29 02:55:54 INFO DAGScheduler: ResultStage 4 (collect at utils.scala:211) finished in 0.011 s
18/06/29 02:55:54 INFO DAGScheduler: Job 2 finished: collect at utils.scala:211, took 0.810144 s
18/06/29 02:55:54 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
18/06/29 02:55:54 INFO CodeGenerator: Code generated in 10.982841 ms
18/06/29 02:55:54 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 02:55:54 INFO SparkSqlParser: Parsing command: SELECT *
FROM `flights` AS `zzz1`
WHERE (0 = 1)
18/06/29 02:55:55 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 02:55:55 INFO SparkSqlParser: Parsing command: SHOW TABLES
18/06/29 02:55:55 INFO HiveMetaStore: 0: get_database: default
18/06/29 02:55:55 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: default	
18/06/29 02:55:55 INFO HiveMetaStore: 0: get_database: default
18/06/29 02:55:55 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: default	
18/06/29 02:55:55 INFO HiveMetaStore: 0: get_tables: db=default pat=*
18/06/29 02:55:55 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
18/06/29 02:55:55 INFO CodeGenerator: Code generated in 13.765983 ms
18/06/29 02:55:55 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 02:55:55 INFO SparkSqlParser: Parsing command: SHOW TABLES
18/06/29 02:55:55 INFO HiveMetaStore: 0: get_database: default
18/06/29 02:55:55 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: default	
18/06/29 02:55:55 INFO HiveMetaStore: 0: get_database: default
18/06/29 02:55:55 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: default	
18/06/29 02:55:55 INFO HiveMetaStore: 0: get_tables: db=default pat=*
18/06/29 02:55:55 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
18/06/29 02:55:55 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 02:55:55 INFO SparkSqlParser: Parsing command: SHOW TABLES
18/06/29 02:55:55 INFO HiveMetaStore: 0: get_database: default
18/06/29 02:55:55 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: default	
18/06/29 02:55:55 INFO HiveMetaStore: 0: get_database: default
18/06/29 02:55:55 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: default	
18/06/29 02:55:55 INFO HiveMetaStore: 0: get_tables: db=default pat=*
18/06/29 02:55:55 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
18/06/29 02:57:44 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 02:57:44 INFO SparkSqlParser: Parsing command: SHOW TABLES
18/06/29 02:57:44 INFO HiveMetaStore: 0: get_database: default
18/06/29 02:57:44 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: default	
18/06/29 02:57:44 INFO HiveMetaStore: 0: get_database: default
18/06/29 02:57:44 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: default	
18/06/29 02:57:44 INFO HiveMetaStore: 0: get_tables: db=default pat=*
18/06/29 02:57:44 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
18/06/29 02:57:44 INFO CodeGenerator: Code generated in 39.220551 ms
18/06/29 02:57:44 INFO SparkContext: Starting job: collect at utils.scala:58
18/06/29 02:57:44 INFO DAGScheduler: Got job 3 (collect at utils.scala:58) with 1 output partitions
18/06/29 02:57:44 INFO DAGScheduler: Final stage: ResultStage 5 (collect at utils.scala:58)
18/06/29 02:57:44 INFO DAGScheduler: Parents of final stage: List()
18/06/29 02:57:44 INFO DAGScheduler: Missing parents: List()
18/06/29 02:57:44 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[26] at map at utils.scala:55), which has no missing parents
18/06/29 02:57:44 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 6.3 KB, free 343.7 MB)
18/06/29 02:57:44 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 3.5 KB, free 343.7 MB)
18/06/29 02:57:44 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 127.0.0.1:54979 (size: 3.5 KB, free: 343.8 MB)
18/06/29 02:57:44 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1006
18/06/29 02:57:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[26] at map at utils.scala:55) (first 15 tasks are for partitions Vector(0))
18/06/29 02:57:44 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks
18/06/29 02:57:44 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 5011 bytes)
18/06/29 02:57:44 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
18/06/29 02:57:44 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 127.0.0.1:54979 in memory (size: 11.7 KB, free: 343.8 MB)
18/06/29 02:57:45 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 127.0.0.1:54979 in memory (size: 3.7 KB, free: 343.8 MB)
18/06/29 02:57:45 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 937 bytes result sent to driver
18/06/29 02:57:45 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 215 ms on localhost (executor driver) (1/1)
18/06/29 02:57:45 INFO DAGScheduler: ResultStage 5 (collect at utils.scala:58) finished in 0.216 s
18/06/29 02:57:45 INFO DAGScheduler: Job 3 finished: collect at utils.scala:58, took 0.251289 s
18/06/29 02:57:45 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
18/06/29 02:59:46 INFO SparkContext: Running Spark version 2.2.0
18/06/29 02:59:46 INFO SparkContext: Submitted application: sparklyr
18/06/29 02:59:46 INFO SecurityManager: Changing view acls to: tehno-gate
18/06/29 02:59:46 INFO SecurityManager: Changing modify acls to: tehno-gate
18/06/29 02:59:46 INFO SecurityManager: Changing view acls groups to: 
18/06/29 02:59:46 INFO SecurityManager: Changing modify acls groups to: 
18/06/29 02:59:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tehno-gate); groups with view permissions: Set(); users  with modify permissions: Set(tehno-gate); groups with modify permissions: Set()
18/06/29 02:59:46 INFO Utils: Successfully started service 'sparkDriver' on port 55031.
18/06/29 02:59:46 INFO SparkEnv: Registering MapOutputTracker
18/06/29 02:59:46 INFO SparkEnv: Registering BlockManagerMaster
18/06/29 02:59:46 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
18/06/29 02:59:46 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
18/06/29 02:59:47 INFO DiskBlockManager: Created local directory at C:\Users\tehno-gate\AppData\Local\Temp\blockmgr-c1846ae6-d41f-4b9e-bb94-6b7f513a6aa0
18/06/29 02:59:47 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
18/06/29 02:59:47 INFO SparkEnv: Registering OutputCommitCoordinator
18/06/29 02:59:47 INFO Utils: Successfully started service 'SparkUI' on port 4040.
18/06/29 02:59:47 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://127.0.0.1:4040
18/06/29 02:59:47 INFO SparkContext: Added JAR file:/C:/Users/tehno-gate/Documents/R/win-library/3.4/sparklyr/java/sparklyr-2.2-2.11.jar at spark://127.0.0.1:55031/jars/sparklyr-2.2-2.11.jar with timestamp 1530230387780
18/06/29 02:59:47 INFO Executor: Starting executor ID driver on host localhost
18/06/29 02:59:47 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55052.
18/06/29 02:59:47 INFO NettyBlockTransferService: Server created on 127.0.0.1:55052
18/06/29 02:59:47 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
18/06/29 02:59:47 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 55052, None)
18/06/29 02:59:47 INFO BlockManagerMasterEndpoint: Registering block manager 127.0.0.1:55052 with 366.3 MB RAM, BlockManagerId(driver, 127.0.0.1, 55052, None)
18/06/29 02:59:47 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 55052, None)
18/06/29 02:59:47 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 127.0.0.1, 55052, None)
18/06/29 02:59:48 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.
18/06/29 02:59:48 INFO SharedState: loading hive config file: file:/C:/Users/tehno-gate/AppData/Local/spark/spark-2.2.0-bin-hadoop2.7/conf/hive-site.xml
18/06/29 02:59:48 INFO SharedState: Setting hive.metastore.warehouse.dir ('C:\Users\tehno-gate\AppData\Local\spark\spark-2.2.0-bin-hadoop2.7\tmp\hive') to the value of spark.sql.warehouse.dir ('C:Users	ehno-gateAppDataLocalsparkspark-2.2.0-bin-hadoop2.7	mphive').
18/06/29 02:59:48 INFO SharedState: Warehouse path is 'C:Users	ehno-gateAppDataLocalsparkspark-2.2.0-bin-hadoop2.7	mphive'.
18/06/29 02:59:50 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
18/06/29 02:59:51 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
18/06/29 02:59:51 INFO ObjectStore: ObjectStore, initialize called
18/06/29 02:59:51 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
18/06/29 02:59:51 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
18/06/29 02:59:53 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
18/06/29 02:59:54 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
18/06/29 02:59:54 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
18/06/29 02:59:55 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
18/06/29 02:59:55 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
18/06/29 02:59:56 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
18/06/29 02:59:56 INFO ObjectStore: Initialized ObjectStore
18/06/29 02:59:56 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
18/06/29 02:59:56 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
18/06/29 02:59:56 INFO HiveMetaStore: Added admin role in metastore
18/06/29 02:59:56 INFO HiveMetaStore: Added public role in metastore
18/06/29 02:59:56 INFO HiveMetaStore: No user is added in admin role, since config is empty
18/06/29 02:59:56 INFO HiveMetaStore: 0: get_all_databases
18/06/29 02:59:56 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_all_databases	
18/06/29 02:59:56 INFO HiveMetaStore: 0: get_functions: db=default pat=*
18/06/29 02:59:56 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
18/06/29 02:59:56 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
18/06/29 02:59:56 INFO SessionState: Created local directory: C:/Users/TEHNO-~1/AppData/Local/Temp/edbf82b4-e7dd-4a5b-8b91-f128e8c789e2_resources
18/06/29 02:59:56 INFO SessionState: Created HDFS directory: C:/Users/tehno-gate/AppData/Local/spark/spark-2.2.0-bin-hadoop2.7/tmp/hive/tehno-gate/edbf82b4-e7dd-4a5b-8b91-f128e8c789e2
18/06/29 02:59:56 INFO SessionState: Created local directory: C:/Users/tehno-gate/AppData/Local/spark/spark-2.2.0-bin-hadoop2.7/tmp/hive/edbf82b4-e7dd-4a5b-8b91-f128e8c789e2
18/06/29 02:59:56 INFO SessionState: Created HDFS directory: C:/Users/tehno-gate/AppData/Local/spark/spark-2.2.0-bin-hadoop2.7/tmp/hive/tehno-gate/edbf82b4-e7dd-4a5b-8b91-f128e8c789e2/_tmp_space.db
18/06/29 02:59:56 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is C:Users	ehno-gateAppDataLocalsparkspark-2.2.0-bin-hadoop2.7	mphive
18/06/29 02:59:57 INFO HiveMetaStore: 0: get_database: default
18/06/29 02:59:57 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: default	
18/06/29 02:59:57 INFO HiveMetaStore: 0: get_database: global_temp
18/06/29 02:59:57 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: global_temp	
18/06/29 02:59:57 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
18/06/29 02:59:57 INFO SessionState: Created local directory: C:/Users/TEHNO-~1/AppData/Local/Temp/8613fcb0-0477-4d0c-bc4c-52b3d52cea4d_resources
18/06/29 02:59:57 INFO SessionState: Created HDFS directory: C:/Users/tehno-gate/AppData/Local/spark/spark-2.2.0-bin-hadoop2.7/tmp/hive/tehno-gate/8613fcb0-0477-4d0c-bc4c-52b3d52cea4d
18/06/29 02:59:57 INFO SessionState: Created local directory: C:/Users/tehno-gate/AppData/Local/spark/spark-2.2.0-bin-hadoop2.7/tmp/hive/8613fcb0-0477-4d0c-bc4c-52b3d52cea4d
18/06/29 02:59:57 INFO SessionState: Created HDFS directory: C:/Users/tehno-gate/AppData/Local/spark/spark-2.2.0-bin-hadoop2.7/tmp/hive/tehno-gate/8613fcb0-0477-4d0c-bc4c-52b3d52cea4d/_tmp_space.db
18/06/29 02:59:57 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is C:Users	ehno-gateAppDataLocalsparkspark-2.2.0-bin-hadoop2.7	mphive
18/06/29 02:59:57 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
18/06/29 02:59:57 INFO SparkSqlParser: Parsing command: SHOW TABLES
18/06/29 02:59:59 INFO HiveMetaStore: 0: get_database: default
18/06/29 02:59:59 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: default	
18/06/29 02:59:59 INFO HiveMetaStore: 0: get_database: default
18/06/29 02:59:59 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: default	
18/06/29 02:59:59 INFO HiveMetaStore: 0: get_tables: db=default pat=*
18/06/29 02:59:59 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
18/06/29 03:00:00 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 03:00:00 INFO SparkSqlParser: Parsing command: SHOW TABLES
18/06/29 03:00:00 INFO HiveMetaStore: 0: get_database: default
18/06/29 03:00:00 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: default	
18/06/29 03:00:00 INFO HiveMetaStore: 0: get_database: default
18/06/29 03:00:00 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: default	
18/06/29 03:00:00 INFO HiveMetaStore: 0: get_tables: db=default pat=*
18/06/29 03:00:00 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
18/06/29 03:00:01 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 03:00:01 INFO SparkSqlParser: Parsing command: SHOW TABLES
18/06/29 03:00:01 INFO HiveMetaStore: 0: get_database: default
18/06/29 03:00:01 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: default	
18/06/29 03:00:01 INFO HiveMetaStore: 0: get_database: default
18/06/29 03:00:01 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: default	
18/06/29 03:00:01 INFO HiveMetaStore: 0: get_tables: db=default pat=*
18/06/29 03:00:01 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
18/06/29 03:00:01 INFO SparkContext: Starting job: collect at utils.scala:58
18/06/29 03:00:01 INFO DAGScheduler: Got job 0 (collect at utils.scala:58) with 1 output partitions
18/06/29 03:00:01 INFO DAGScheduler: Final stage: ResultStage 0 (collect at utils.scala:58)
18/06/29 03:00:01 INFO DAGScheduler: Parents of final stage: List()
18/06/29 03:00:01 INFO DAGScheduler: Missing parents: List()
18/06/29 03:00:01 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[4] at map at utils.scala:55), which has no missing parents
18/06/29 03:00:02 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 6.0 KB, free 366.3 MB)
18/06/29 03:00:02 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.4 KB, free 366.3 MB)
18/06/29 03:00:02 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 127.0.0.1:55052 (size: 3.4 KB, free: 366.3 MB)
18/06/29 03:00:02 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
18/06/29 03:00:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at map at utils.scala:55) (first 15 tasks are for partitions Vector(0))
18/06/29 03:00:02 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
18/06/29 03:00:02 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4852 bytes)
18/06/29 03:00:02 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
18/06/29 03:00:02 INFO Executor: Fetching spark://127.0.0.1:55031/jars/sparklyr-2.2-2.11.jar with timestamp 1530230387780
18/06/29 03:00:02 INFO TransportClientFactory: Successfully created connection to /127.0.0.1:55031 after 29 ms (0 ms spent in bootstraps)
18/06/29 03:00:02 INFO Utils: Fetching spark://127.0.0.1:55031/jars/sparklyr-2.2-2.11.jar to C:\Users\tehno-gate\AppData\Local\Temp\spark-3a472ccf-0b23-452d-9763-1dc5717bed9a\userFiles-7940d89f-61fd-4bbc-90a4-d32482331b58\fetchFileTemp2022278752798407615.tmp
18/06/29 03:00:02 INFO Executor: Adding file:/C:/Users/tehno-gate/AppData/Local/Temp/spark-3a472ccf-0b23-452d-9763-1dc5717bed9a/userFiles-7940d89f-61fd-4bbc-90a4-d32482331b58/sparklyr-2.2-2.11.jar to class loader
18/06/29 03:00:03 INFO CodeGenerator: Code generated in 342.003934 ms
18/06/29 03:00:03 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1013 bytes result sent to driver
18/06/29 03:00:03 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 980 ms on localhost (executor driver) (1/1)
18/06/29 03:00:03 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
18/06/29 03:00:03 INFO DAGScheduler: ResultStage 0 (collect at utils.scala:58) finished in 1.006 s
18/06/29 03:00:03 INFO DAGScheduler: Job 0 finished: collect at utils.scala:58, took 1.256484 s
18/06/29 03:00:17 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 03:00:17 INFO SparkSqlParser: Parsing command: flights
18/06/29 03:00:17 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 03:00:17 INFO SparkSqlParser: Parsing command: CACHE TABLE `flights`
18/06/29 03:00:18 INFO SparkSqlParser: Parsing command: `flights`
18/06/29 03:00:18 INFO CodeGenerator: Code generated in 26.399992 ms
18/06/29 03:00:18 INFO CodeGenerator: Code generated in 17.129296 ms
18/06/29 03:00:18 INFO SparkContext: Starting job: sql at NativeMethodAccessorImpl.java:0
18/06/29 03:00:18 INFO DAGScheduler: Registering RDD 12 (sql at NativeMethodAccessorImpl.java:0)
18/06/29 03:00:18 INFO DAGScheduler: Got job 1 (sql at NativeMethodAccessorImpl.java:0) with 1 output partitions
18/06/29 03:00:18 INFO DAGScheduler: Final stage: ResultStage 2 (sql at NativeMethodAccessorImpl.java:0)
18/06/29 03:00:18 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
18/06/29 03:00:18 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)
18/06/29 03:00:18 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[12] at sql at NativeMethodAccessorImpl.java:0), which has no missing parents
18/06/29 03:00:18 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 30.7 KB, free 366.3 MB)
18/06/29 03:00:18 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 11.8 KB, free 366.2 MB)
18/06/29 03:00:18 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 127.0.0.1:55052 (size: 11.8 KB, free: 366.3 MB)
18/06/29 03:00:18 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
18/06/29 03:00:18 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[12] at sql at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
18/06/29 03:00:18 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
18/06/29 03:00:19 INFO ContextCleaner: Cleaned accumulator 51
18/06/29 03:00:19 INFO ContextCleaner: Cleaned accumulator 0
18/06/29 03:00:19 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 127.0.0.1:55052 in memory (size: 3.4 KB, free: 366.3 MB)
18/06/29 03:00:19 WARN TaskSetManager: Stage 1 contains a task of very large size (27699 KB). The maximum recommended task size is 100 KB.
18/06/29 03:00:19 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 28364103 bytes)
18/06/29 03:00:19 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
18/06/29 03:00:19 INFO CodeGenerator: Code generated in 42.189637 ms
18/06/29 03:00:20 INFO CodeGenerator: Code generated in 216.219855 ms
18/06/29 03:00:25 INFO MemoryStore: Block rdd_9_0 stored as values in memory (estimated size 22.5 MB, free 343.8 MB)
18/06/29 03:00:25 INFO BlockManagerInfo: Added rdd_9_0 in memory on 127.0.0.1:55052 (size: 22.5 MB, free: 343.8 MB)
18/06/29 03:00:25 INFO CodeGenerator: Code generated in 10.601291 ms
18/06/29 03:00:25 INFO CodeGenerator: Code generated in 28.709818 ms
18/06/29 03:00:25 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2371 bytes result sent to driver
18/06/29 03:00:25 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 7380 ms on localhost (executor driver) (1/1)
18/06/29 03:00:25 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
18/06/29 03:00:25 INFO DAGScheduler: ShuffleMapStage 1 (sql at NativeMethodAccessorImpl.java:0) finished in 7.383 s
18/06/29 03:00:25 INFO DAGScheduler: looking for newly runnable stages
18/06/29 03:00:25 INFO DAGScheduler: running: Set()
18/06/29 03:00:25 INFO DAGScheduler: waiting: Set(ResultStage 2)
18/06/29 03:00:25 INFO DAGScheduler: failed: Set()
18/06/29 03:00:25 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[15] at sql at NativeMethodAccessorImpl.java:0), which has no missing parents
18/06/29 03:00:25 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 7.0 KB, free 343.8 MB)
18/06/29 03:00:25 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.7 KB, free 343.8 MB)
18/06/29 03:00:25 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 127.0.0.1:55052 (size: 3.7 KB, free: 343.8 MB)
18/06/29 03:00:25 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
18/06/29 03:00:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[15] at sql at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
18/06/29 03:00:25 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
18/06/29 03:00:25 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 4726 bytes)
18/06/29 03:00:25 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
18/06/29 03:00:25 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
18/06/29 03:00:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
18/06/29 03:00:25 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1538 bytes result sent to driver
18/06/29 03:00:25 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 49 ms on localhost (executor driver) (1/1)
18/06/29 03:00:25 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
18/06/29 03:00:25 INFO DAGScheduler: ResultStage 2 (sql at NativeMethodAccessorImpl.java:0) finished in 0.051 s
18/06/29 03:00:25 INFO DAGScheduler: Job 1 finished: sql at NativeMethodAccessorImpl.java:0, took 7.532726 s
18/06/29 03:00:25 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 03:00:26 INFO SparkSqlParser: Parsing command: SELECT count(*) FROM  `flights`
18/06/29 03:00:26 INFO HiveMetaStore: 0: get_database: default
18/06/29 03:00:26 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: default	
18/06/29 03:00:26 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 127.0.0.1:55052 in memory (size: 3.7 KB, free: 343.8 MB)
18/06/29 03:00:26 INFO SparkContext: Starting job: collect at utils.scala:211
18/06/29 03:00:26 INFO DAGScheduler: Registering RDD 18 (collect at utils.scala:211)
18/06/29 03:00:26 INFO DAGScheduler: Got job 2 (collect at utils.scala:211) with 1 output partitions
18/06/29 03:00:26 INFO DAGScheduler: Final stage: ResultStage 4 (collect at utils.scala:211)
18/06/29 03:00:26 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
18/06/29 03:00:26 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 3)
18/06/29 03:00:26 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[18] at collect at utils.scala:211), which has no missing parents
18/06/29 03:00:26 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 30.7 KB, free 343.7 MB)
18/06/29 03:00:26 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 11.7 KB, free 343.7 MB)
18/06/29 03:00:26 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 127.0.0.1:55052 (size: 11.7 KB, free: 343.8 MB)
18/06/29 03:00:26 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1006
18/06/29 03:00:26 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[18] at collect at utils.scala:211) (first 15 tasks are for partitions Vector(0))
18/06/29 03:00:26 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
18/06/29 03:00:26 WARN TaskSetManager: Stage 3 contains a task of very large size (27699 KB). The maximum recommended task size is 100 KB.
18/06/29 03:00:26 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 28364103 bytes)
18/06/29 03:00:26 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
18/06/29 03:00:26 INFO ContextCleaner: Cleaned accumulator 112
18/06/29 03:00:26 INFO BlockManager: Found block rdd_9_0 locally
18/06/29 03:00:26 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1733 bytes result sent to driver
18/06/29 03:00:26 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 712 ms on localhost (executor driver) (1/1)
18/06/29 03:00:26 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
18/06/29 03:00:26 INFO DAGScheduler: ShuffleMapStage 3 (collect at utils.scala:211) finished in 0.713 s
18/06/29 03:00:26 INFO DAGScheduler: looking for newly runnable stages
18/06/29 03:00:26 INFO DAGScheduler: running: Set()
18/06/29 03:00:26 INFO DAGScheduler: waiting: Set(ResultStage 4)
18/06/29 03:00:26 INFO DAGScheduler: failed: Set()
18/06/29 03:00:26 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[21] at collect at utils.scala:211), which has no missing parents
18/06/29 03:00:26 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 7.0 KB, free 343.7 MB)
18/06/29 03:00:26 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.7 KB, free 343.7 MB)
18/06/29 03:00:26 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 127.0.0.1:55052 (size: 3.7 KB, free: 343.8 MB)
18/06/29 03:00:26 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1006
18/06/29 03:00:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[21] at collect at utils.scala:211) (first 15 tasks are for partitions Vector(0))
18/06/29 03:00:26 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
18/06/29 03:00:26 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, ANY, 4726 bytes)
18/06/29 03:00:26 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
18/06/29 03:00:26 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
18/06/29 03:00:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
18/06/29 03:00:26 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1538 bytes result sent to driver
18/06/29 03:00:26 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 10 ms on localhost (executor driver) (1/1)
18/06/29 03:00:26 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
18/06/29 03:00:26 INFO DAGScheduler: ResultStage 4 (collect at utils.scala:211) finished in 0.012 s
18/06/29 03:00:26 INFO DAGScheduler: Job 2 finished: collect at utils.scala:211, took 0.759621 s
18/06/29 03:00:26 INFO CodeGenerator: Code generated in 9.8883 ms
18/06/29 03:00:27 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 03:00:27 INFO SparkSqlParser: Parsing command: SELECT *
FROM `flights` AS `zzz1`
WHERE (0 = 1)
18/06/29 03:00:27 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 03:00:27 INFO SparkSqlParser: Parsing command: SHOW TABLES
18/06/29 03:00:27 INFO HiveMetaStore: 0: get_database: default
18/06/29 03:00:27 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: default	
18/06/29 03:00:27 INFO HiveMetaStore: 0: get_database: default
18/06/29 03:00:27 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: default	
18/06/29 03:00:27 INFO HiveMetaStore: 0: get_tables: db=default pat=*
18/06/29 03:00:27 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
18/06/29 03:00:27 INFO CodeGenerator: Code generated in 12.923795 ms
18/06/29 03:00:28 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 03:00:28 INFO SparkSqlParser: Parsing command: SELECT *
FROM `flights`
18/06/29 03:00:29 INFO SparkSqlParser: Parsing command: sparklyr_tmp_593c69f03d0
18/06/29 03:00:29 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 03:00:29 INFO SparkSqlParser: Parsing command: SELECT *
FROM `sparklyr_tmp_593c69f03d0` AS `zzz2`
WHERE (0 = 1)
18/06/29 03:00:29 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 03:00:29 INFO SparkSqlParser: Parsing command: SELECT *
FROM `sparklyr_tmp_593c69f03d0`
18/06/29 03:00:29 INFO CodeGenerator: Code generated in 44.819434 ms
18/06/29 03:00:29 INFO SparkContext: Starting job: collect at utils.scala:211
18/06/29 03:00:29 INFO DAGScheduler: Got job 3 (collect at utils.scala:211) with 1 output partitions
18/06/29 03:00:29 INFO DAGScheduler: Final stage: ResultStage 5 (collect at utils.scala:211)
18/06/29 03:00:29 INFO DAGScheduler: Parents of final stage: List()
18/06/29 03:00:29 INFO DAGScheduler: Missing parents: List()
18/06/29 03:00:29 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[24] at collect at utils.scala:211), which has no missing parents
18/06/29 03:00:29 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 39.5 KB, free 343.7 MB)
18/06/29 03:00:29 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 14.9 KB, free 343.7 MB)
18/06/29 03:00:29 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 127.0.0.1:55052 (size: 14.9 KB, free: 343.8 MB)
18/06/29 03:00:29 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1006
18/06/29 03:00:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[24] at collect at utils.scala:211) (first 15 tasks are for partitions Vector(0))
18/06/29 03:00:29 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks
18/06/29 03:00:29 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 127.0.0.1:55052 in memory (size: 11.7 KB, free: 343.8 MB)
18/06/29 03:00:29 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 127.0.0.1:55052 in memory (size: 3.7 KB, free: 343.8 MB)
18/06/29 03:00:29 WARN TaskSetManager: Stage 5 contains a task of very large size (27699 KB). The maximum recommended task size is 100 KB.
18/06/29 03:00:29 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 28364114 bytes)
18/06/29 03:00:29 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
18/06/29 03:00:29 INFO BlockManager: Found block rdd_9_0 locally
18/06/29 03:00:29 INFO CodeGenerator: Code generated in 47.659324 ms
18/06/29 03:00:31 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 127.0.0.1:55052 in memory (size: 11.8 KB, free: 343.8 MB)
18/06/29 03:00:31 INFO ContextCleaner: Cleaned accumulator 61
18/06/29 03:00:31 INFO ContextCleaner: Cleaned accumulator 63
18/06/29 03:00:31 INFO ContextCleaner: Cleaned accumulator 53
18/06/29 03:00:31 INFO ContextCleaner: Cleaned accumulator 59
18/06/29 03:00:31 INFO ContextCleaner: Cleaned accumulator 57
18/06/29 03:00:31 INFO ContextCleaner: Cleaned accumulator 56
18/06/29 03:00:31 INFO ContextCleaner: Cleaned accumulator 58
18/06/29 03:00:31 INFO ContextCleaner: Cleaned accumulator 54
18/06/29 03:00:31 INFO ContextCleaner: Cleaned accumulator 60
18/06/29 03:00:31 INFO ContextCleaner: Cleaned accumulator 52
18/06/29 03:00:31 INFO ContextCleaner: Cleaned shuffle 0
18/06/29 03:00:31 INFO ContextCleaner: Cleaned accumulator 62
18/06/29 03:00:31 INFO ContextCleaner: Cleaned accumulator 55
18/06/29 03:00:31 INFO MemoryStore: Block taskresult_5 stored as bytes in memory (estimated size 19.2 MB, free 324.6 MB)
18/06/29 03:00:31 INFO BlockManagerInfo: Added taskresult_5 in memory on 127.0.0.1:55052 (size: 19.2 MB, free: 324.6 MB)
18/06/29 03:00:31 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 20122941 bytes result sent via BlockManager)
18/06/29 03:00:31 INFO TransportClientFactory: Successfully created connection to /127.0.0.1:55052 after 3 ms (0 ms spent in bootstraps)
18/06/29 03:00:31 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 2577 ms on localhost (executor driver) (1/1)
18/06/29 03:00:31 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
18/06/29 03:00:31 INFO DAGScheduler: ResultStage 5 (collect at utils.scala:211) finished in 2.579 s
18/06/29 03:00:31 INFO DAGScheduler: Job 3 finished: collect at utils.scala:211, took 2.594882 s
18/06/29 03:00:31 INFO BlockManagerInfo: Removed taskresult_5 on 127.0.0.1:55052 in memory (size: 19.2 MB, free: 343.8 MB)
18/06/29 03:00:32 INFO CodeGenerator: Code generated in 34.93536 ms
18/06/29 03:00:33 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 127.0.0.1:55052 in memory (size: 14.9 KB, free: 343.8 MB)
18/06/29 03:04:28 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 03:04:28 INFO SparkSqlParser: Parsing command: SELECT *
FROM `flights`
18/06/29 03:04:28 INFO SparkSqlParser: Parsing command: sparklyr_tmp_593c7ee63fbf
18/06/29 03:04:28 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 03:04:28 INFO SparkSqlParser: Parsing command: SELECT *
FROM `sparklyr_tmp_593c7ee63fbf` AS `zzz3`
WHERE (0 = 1)
18/06/29 03:04:28 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 03:04:28 INFO SparkSqlParser: Parsing command: SELECT *
FROM `sparklyr_tmp_593c7ee63fbf`
18/06/29 03:04:28 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 03:04:28 INFO SparkSqlParser: Parsing command: SELECT *
FROM `sparklyr_tmp_593c7ee63fbf`
LIMIT 10
18/06/29 03:04:29 INFO SparkContext: Starting job: collect at utils.scala:211
18/06/29 03:04:29 INFO DAGScheduler: Got job 4 (collect at utils.scala:211) with 1 output partitions
18/06/29 03:04:29 INFO DAGScheduler: Final stage: ResultStage 6 (collect at utils.scala:211)
18/06/29 03:04:29 INFO DAGScheduler: Parents of final stage: List()
18/06/29 03:04:29 INFO DAGScheduler: Missing parents: List()
18/06/29 03:04:29 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[27] at collect at utils.scala:211), which has no missing parents
18/06/29 03:04:29 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 39.4 KB, free 343.8 MB)
18/06/29 03:04:29 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 14.9 KB, free 343.8 MB)
18/06/29 03:04:29 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 127.0.0.1:55052 (size: 14.9 KB, free: 343.8 MB)
18/06/29 03:04:29 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1006
18/06/29 03:04:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[27] at collect at utils.scala:211) (first 15 tasks are for partitions Vector(0))
18/06/29 03:04:29 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks
18/06/29 03:04:30 WARN TaskSetManager: Stage 6 contains a task of very large size (27699 KB). The maximum recommended task size is 100 KB.
18/06/29 03:04:30 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 28364114 bytes)
18/06/29 03:04:30 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
18/06/29 03:04:30 INFO BlockManager: Found block rdd_9_0 locally
18/06/29 03:04:30 INFO Executor: 1 block locks were not released by TID = 6:
[rdd_9_0]
18/06/29 03:04:30 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 2077 bytes result sent to driver
18/06/29 03:04:30 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 1695 ms on localhost (executor driver) (1/1)
18/06/29 03:04:30 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
18/06/29 03:04:30 INFO DAGScheduler: ResultStage 6 (collect at utils.scala:211) finished in 1.697 s
18/06/29 03:04:30 INFO DAGScheduler: Job 4 finished: collect at utils.scala:211, took 1.727610 s
18/06/29 03:05:01 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 03:05:01 INFO SparkSqlParser: Parsing command: SELECT *
FROM `flights`
18/06/29 03:05:02 INFO SparkSqlParser: Parsing command: sparklyr_tmp_593c49e034f0
18/06/29 03:05:02 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 03:05:02 INFO SparkSqlParser: Parsing command: SELECT *
FROM `sparklyr_tmp_593c49e034f0` AS `zzz4`
WHERE (0 = 1)
18/06/29 03:05:04 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 03:05:04 INFO SparkSqlParser: Parsing command: SELECT `arr_delay`, `delay_degree`
FROM `sparklyr_tmp_593c49e034f0`
18/06/29 03:05:04 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 03:05:04 INFO SparkSqlParser: Parsing command: SELECT `arr_delay`, `delay_degree`
FROM `sparklyr_tmp_593c49e034f0`
LIMIT 10
18/06/29 03:05:04 INFO CodeGenerator: Code generated in 25.784803 ms
18/06/29 03:05:04 INFO SparkContext: Starting job: collect at utils.scala:211
18/06/29 03:05:04 INFO DAGScheduler: Got job 5 (collect at utils.scala:211) with 1 output partitions
18/06/29 03:05:04 INFO DAGScheduler: Final stage: ResultStage 7 (collect at utils.scala:211)
18/06/29 03:05:04 INFO DAGScheduler: Parents of final stage: List()
18/06/29 03:05:04 INFO DAGScheduler: Missing parents: List()
18/06/29 03:05:04 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at utils.scala:211), which has no missing parents
18/06/29 03:05:04 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 33.7 KB, free 343.7 MB)
18/06/29 03:05:04 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 13.5 KB, free 343.7 MB)
18/06/29 03:05:04 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 127.0.0.1:55052 (size: 13.5 KB, free: 343.8 MB)
18/06/29 03:05:04 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1006
18/06/29 03:05:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at utils.scala:211) (first 15 tasks are for partitions Vector(0))
18/06/29 03:05:04 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks
18/06/29 03:05:05 WARN TaskSetManager: Stage 7 contains a task of very large size (27699 KB). The maximum recommended task size is 100 KB.
18/06/29 03:05:05 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7, localhost, executor driver, partition 0, PROCESS_LOCAL, 28364114 bytes)
18/06/29 03:05:05 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
18/06/29 03:05:05 INFO BlockManager: Found block rdd_9_0 locally
18/06/29 03:05:05 INFO CodeGenerator: Code generated in 39.684811 ms
18/06/29 03:05:05 INFO Executor: 1 block locks were not released by TID = 7:
[rdd_9_0]
18/06/29 03:05:05 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1420 bytes result sent to driver
18/06/29 03:05:05 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 743 ms on localhost (executor driver) (1/1)
18/06/29 03:05:05 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
18/06/29 03:05:05 INFO DAGScheduler: ResultStage 7 (collect at utils.scala:211) finished in 0.745 s
18/06/29 03:05:05 INFO DAGScheduler: Job 5 finished: collect at utils.scala:211, took 0.758591 s
18/06/29 03:05:05 INFO CodeGenerator: Code generated in 10.68883 ms
18/06/29 03:13:16 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 03:13:17 INFO SparkSqlParser: Parsing command: SHOW TABLES
18/06/29 03:13:17 INFO HiveMetaStore: 0: get_database: default
18/06/29 03:13:17 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: default	
18/06/29 03:13:17 INFO HiveMetaStore: 0: get_database: default
18/06/29 03:13:17 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: default	
18/06/29 03:13:17 INFO HiveMetaStore: 0: get_tables: db=default pat=*
18/06/29 03:13:17 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
18/06/29 03:13:17 INFO CodeGenerator: Code generated in 59.483748 ms
18/06/29 03:13:17 INFO SparkContext: Starting job: collect at utils.scala:58
18/06/29 03:13:17 INFO DAGScheduler: Got job 6 (collect at utils.scala:58) with 4 output partitions
18/06/29 03:13:17 INFO DAGScheduler: Final stage: ResultStage 8 (collect at utils.scala:58)
18/06/29 03:13:17 INFO DAGScheduler: Parents of final stage: List()
18/06/29 03:13:17 INFO DAGScheduler: Missing parents: List()
18/06/29 03:13:17 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[35] at map at utils.scala:55), which has no missing parents
18/06/29 03:13:17 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 6.6 KB, free 343.7 MB)
18/06/29 03:13:17 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 3.6 KB, free 343.7 MB)
18/06/29 03:13:17 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 127.0.0.1:55052 (size: 3.6 KB, free: 343.8 MB)
18/06/29 03:13:17 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1006
18/06/29 03:13:17 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 8 (MapPartitionsRDD[35] at map at utils.scala:55) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
18/06/29 03:13:17 INFO TaskSchedulerImpl: Adding task set 8.0 with 4 tasks
18/06/29 03:13:17 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8, localhost, executor driver, partition 0, PROCESS_LOCAL, 5011 bytes)
18/06/29 03:13:17 INFO TaskSetManager: Starting task 1.0 in stage 8.0 (TID 9, localhost, executor driver, partition 1, PROCESS_LOCAL, 5035 bytes)
18/06/29 03:13:17 INFO TaskSetManager: Starting task 2.0 in stage 8.0 (TID 10, localhost, executor driver, partition 2, PROCESS_LOCAL, 5027 bytes)
18/06/29 03:13:17 INFO TaskSetManager: Starting task 3.0 in stage 8.0 (TID 11, localhost, executor driver, partition 3, PROCESS_LOCAL, 5035 bytes)
18/06/29 03:13:17 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
18/06/29 03:13:17 INFO Executor: Running task 1.0 in stage 8.0 (TID 9)
18/06/29 03:13:17 INFO Executor: Running task 2.0 in stage 8.0 (TID 10)
18/06/29 03:13:17 INFO Executor: Running task 3.0 in stage 8.0 (TID 11)
18/06/29 03:13:17 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 937 bytes result sent to driver
18/06/29 03:13:17 INFO Executor: Finished task 3.0 in stage 8.0 (TID 11). 955 bytes result sent to driver
18/06/29 03:13:17 INFO Executor: Finished task 1.0 in stage 8.0 (TID 9). 912 bytes result sent to driver
18/06/29 03:13:17 INFO TaskSetManager: Finished task 1.0 in stage 8.0 (TID 9) in 46 ms on localhost (executor driver) (1/4)
18/06/29 03:13:17 INFO TaskSetManager: Finished task 3.0 in stage 8.0 (TID 11) in 47 ms on localhost (executor driver) (2/4)
18/06/29 03:13:17 INFO Executor: Finished task 2.0 in stage 8.0 (TID 10). 954 bytes result sent to driver
18/06/29 03:13:17 INFO TaskSetManager: Finished task 2.0 in stage 8.0 (TID 10) in 51 ms on localhost (executor driver) (3/4)
18/06/29 03:13:17 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 63 ms on localhost (executor driver) (4/4)
18/06/29 03:13:17 INFO DAGScheduler: ResultStage 8 (collect at utils.scala:58) finished in 0.065 s
18/06/29 03:13:17 INFO DAGScheduler: Job 6 finished: collect at utils.scala:58, took 0.113105 s
18/06/29 03:13:17 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
18/06/29 03:13:29 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 03:13:29 INFO SparkSqlParser: Parsing command: SHOW TABLES
18/06/29 03:13:29 INFO HiveMetaStore: 0: get_database: default
18/06/29 03:13:29 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: default	
18/06/29 03:13:29 INFO HiveMetaStore: 0: get_database: default
18/06/29 03:13:29 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: default	
18/06/29 03:13:29 INFO HiveMetaStore: 0: get_tables: db=default pat=*
18/06/29 03:13:29 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
18/06/29 03:13:29 INFO SparkContext: Starting job: collect at utils.scala:58
18/06/29 03:13:29 INFO DAGScheduler: Got job 7 (collect at utils.scala:58) with 4 output partitions
18/06/29 03:13:29 INFO DAGScheduler: Final stage: ResultStage 9 (collect at utils.scala:58)
18/06/29 03:13:29 INFO DAGScheduler: Parents of final stage: List()
18/06/29 03:13:29 INFO DAGScheduler: Missing parents: List()
18/06/29 03:13:29 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[40] at map at utils.scala:55), which has no missing parents
18/06/29 03:13:29 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 6.6 KB, free 343.7 MB)
18/06/29 03:13:29 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 3.6 KB, free 343.7 MB)
18/06/29 03:13:29 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 127.0.0.1:55052 (size: 3.6 KB, free: 343.8 MB)
18/06/29 03:13:29 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1006
18/06/29 03:13:29 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 9 (MapPartitionsRDD[40] at map at utils.scala:55) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
18/06/29 03:13:29 INFO TaskSchedulerImpl: Adding task set 9.0 with 4 tasks
18/06/29 03:13:29 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 12, localhost, executor driver, partition 0, PROCESS_LOCAL, 5011 bytes)
18/06/29 03:13:29 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 13, localhost, executor driver, partition 1, PROCESS_LOCAL, 5035 bytes)
18/06/29 03:13:29 INFO TaskSetManager: Starting task 2.0 in stage 9.0 (TID 14, localhost, executor driver, partition 2, PROCESS_LOCAL, 5027 bytes)
18/06/29 03:13:29 INFO TaskSetManager: Starting task 3.0 in stage 9.0 (TID 15, localhost, executor driver, partition 3, PROCESS_LOCAL, 5035 bytes)
18/06/29 03:13:29 INFO Executor: Running task 0.0 in stage 9.0 (TID 12)
18/06/29 03:13:29 INFO Executor: Running task 1.0 in stage 9.0 (TID 13)
18/06/29 03:13:29 INFO Executor: Running task 2.0 in stage 9.0 (TID 14)
18/06/29 03:13:29 INFO Executor: Running task 3.0 in stage 9.0 (TID 15)
18/06/29 03:13:29 INFO Executor: Finished task 1.0 in stage 9.0 (TID 13). 955 bytes result sent to driver
18/06/29 03:13:29 INFO Executor: Finished task 3.0 in stage 9.0 (TID 15). 912 bytes result sent to driver
18/06/29 03:13:29 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 13) in 14 ms on localhost (executor driver) (1/4)
18/06/29 03:13:29 INFO Executor: Finished task 0.0 in stage 9.0 (TID 12). 937 bytes result sent to driver
18/06/29 03:13:29 INFO TaskSetManager: Finished task 3.0 in stage 9.0 (TID 15) in 15 ms on localhost (executor driver) (2/4)
18/06/29 03:13:29 INFO Executor: Finished task 2.0 in stage 9.0 (TID 14). 954 bytes result sent to driver
18/06/29 03:13:29 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 12) in 17 ms on localhost (executor driver) (3/4)
18/06/29 03:13:29 INFO TaskSetManager: Finished task 2.0 in stage 9.0 (TID 14) in 18 ms on localhost (executor driver) (4/4)
18/06/29 03:13:29 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
18/06/29 03:13:29 INFO DAGScheduler: ResultStage 9 (collect at utils.scala:58) finished in 0.020 s
18/06/29 03:13:29 INFO DAGScheduler: Job 7 finished: collect at utils.scala:58, took 0.044644 s
18/06/29 03:26:49 INFO SparkContext: Invoking stop() from shutdown hook
18/06/29 03:26:49 INFO SparkUI: Stopped Spark web UI at http://127.0.0.1:4040
18/06/29 03:26:49 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
18/06/29 03:26:49 INFO MemoryStore: MemoryStore cleared
18/06/29 03:26:49 INFO BlockManager: BlockManager stopped
18/06/29 03:26:49 INFO BlockManagerMaster: BlockManagerMaster stopped
18/06/29 03:26:50 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
18/06/29 03:26:50 WARN SparkEnv: Exception while deleting Spark temp dir: C:\Users\tehno-gate\AppData\Local\Temp\spark-3a472ccf-0b23-452d-9763-1dc5717bed9a\userFiles-7940d89f-61fd-4bbc-90a4-d32482331b58
java.io.IOException: Failed to delete: C:\Users\tehno-gate\AppData\Local\Temp\spark-3a472ccf-0b23-452d-9763-1dc5717bed9a\userFiles-7940d89f-61fd-4bbc-90a4-d32482331b58
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1031)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:103)
	at org.apache.spark.SparkContext$$anonfun$stop$11.apply$mcV$sp(SparkContext.scala:1937)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1936)
	at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:581)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1954)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
18/06/29 03:26:50 INFO SparkContext: Successfully stopped SparkContext
18/06/29 03:26:50 INFO ShutdownHookManager: Shutdown hook called
18/06/29 03:26:50 INFO ShutdownHookManager: Deleting directory C:\Users\tehno-gate\AppData\Local\Temp\spark-3a472ccf-0b23-452d-9763-1dc5717bed9a\userFiles-7940d89f-61fd-4bbc-90a4-d32482331b58
18/06/29 03:26:50 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\tehno-gate\AppData\Local\Temp\spark-3a472ccf-0b23-452d-9763-1dc5717bed9a\userFiles-7940d89f-61fd-4bbc-90a4-d32482331b58
java.io.IOException: Failed to delete: C:\Users\tehno-gate\AppData\Local\Temp\spark-3a472ccf-0b23-452d-9763-1dc5717bed9a\userFiles-7940d89f-61fd-4bbc-90a4-d32482331b58
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1031)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1954)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
18/06/29 03:26:50 INFO ShutdownHookManager: Deleting directory C:\Users\tehno-gate\AppData\Local\Temp\spark-3a472ccf-0b23-452d-9763-1dc5717bed9a
18/06/29 03:26:50 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\tehno-gate\AppData\Local\Temp\spark-3a472ccf-0b23-452d-9763-1dc5717bed9a
java.io.IOException: Failed to delete: C:\Users\tehno-gate\AppData\Local\Temp\spark-3a472ccf-0b23-452d-9763-1dc5717bed9a
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1031)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1954)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
18/06/29 03:27:48 INFO SparkContext: Running Spark version 2.2.0
18/06/29 03:27:49 INFO SparkContext: Submitted application: sparklyr
18/06/29 03:27:49 INFO SecurityManager: Changing view acls to: tehno-gate
18/06/29 03:27:49 INFO SecurityManager: Changing modify acls to: tehno-gate
18/06/29 03:27:49 INFO SecurityManager: Changing view acls groups to: 
18/06/29 03:27:49 INFO SecurityManager: Changing modify acls groups to: 
18/06/29 03:27:49 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tehno-gate); groups with view permissions: Set(); users  with modify permissions: Set(tehno-gate); groups with modify permissions: Set()
18/06/29 03:27:49 INFO Utils: Successfully started service 'sparkDriver' on port 55622.
18/06/29 03:27:49 INFO SparkEnv: Registering MapOutputTracker
18/06/29 03:27:49 INFO SparkEnv: Registering BlockManagerMaster
18/06/29 03:27:49 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
18/06/29 03:27:49 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
18/06/29 03:27:49 INFO DiskBlockManager: Created local directory at C:\Users\tehno-gate\AppData\Local\Temp\blockmgr-ce22bd15-75eb-4e5c-9ede-9ccb1e9bfccd
18/06/29 03:27:49 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
18/06/29 03:27:50 INFO SparkEnv: Registering OutputCommitCoordinator
18/06/29 03:27:50 INFO Utils: Successfully started service 'SparkUI' on port 4040.
18/06/29 03:27:50 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://127.0.0.1:4040
18/06/29 03:27:50 INFO SparkContext: Added JAR file:/C:/Users/tehno-gate/Documents/R/win-library/3.4/sparklyr/java/sparklyr-2.2-2.11.jar at spark://127.0.0.1:55622/jars/sparklyr-2.2-2.11.jar with timestamp 1530232070629
18/06/29 03:27:50 INFO Executor: Starting executor ID driver on host localhost
18/06/29 03:27:50 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55643.
18/06/29 03:27:50 INFO NettyBlockTransferService: Server created on 127.0.0.1:55643
18/06/29 03:27:50 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
18/06/29 03:27:50 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 55643, None)
18/06/29 03:27:50 INFO BlockManagerMasterEndpoint: Registering block manager 127.0.0.1:55643 with 366.3 MB RAM, BlockManagerId(driver, 127.0.0.1, 55643, None)
18/06/29 03:27:50 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 55643, None)
18/06/29 03:27:50 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 127.0.0.1, 55643, None)
18/06/29 03:27:51 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.
18/06/29 03:27:51 INFO SharedState: loading hive config file: file:/C:/Users/tehno-gate/AppData/Local/spark/spark-2.2.0-bin-hadoop2.7/conf/hive-site.xml
18/06/29 03:27:51 INFO SharedState: Setting hive.metastore.warehouse.dir ('C:\Users\tehno-gate\AppData\Local\spark\spark-2.2.0-bin-hadoop2.7\tmp\hive') to the value of spark.sql.warehouse.dir ('C:Users	ehno-gateAppDataLocalsparkspark-2.2.0-bin-hadoop2.7	mphive').
18/06/29 03:27:51 INFO SharedState: Warehouse path is 'C:Users	ehno-gateAppDataLocalsparkspark-2.2.0-bin-hadoop2.7	mphive'.
18/06/29 03:27:53 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
18/06/29 03:27:54 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
18/06/29 03:27:54 INFO ObjectStore: ObjectStore, initialize called
18/06/29 03:27:54 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
18/06/29 03:27:54 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
18/06/29 03:27:57 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
18/06/29 03:27:58 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
18/06/29 03:27:58 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
18/06/29 03:27:59 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
18/06/29 03:27:59 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
18/06/29 03:28:00 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
18/06/29 03:28:00 INFO ObjectStore: Initialized ObjectStore
18/06/29 03:28:00 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
18/06/29 03:28:00 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
18/06/29 03:28:00 INFO HiveMetaStore: Added admin role in metastore
18/06/29 03:28:00 INFO HiveMetaStore: Added public role in metastore
18/06/29 03:28:00 INFO HiveMetaStore: No user is added in admin role, since config is empty
18/06/29 03:28:00 INFO HiveMetaStore: 0: get_all_databases
18/06/29 03:28:00 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_all_databases	
18/06/29 03:28:01 INFO HiveMetaStore: 0: get_functions: db=default pat=*
18/06/29 03:28:01 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
18/06/29 03:28:01 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
18/06/29 03:28:01 INFO SessionState: Created local directory: C:/Users/TEHNO-~1/AppData/Local/Temp/89b35f50-56fc-446d-b56e-5e04534c0b33_resources
18/06/29 03:28:01 INFO SessionState: Created HDFS directory: C:/Users/tehno-gate/AppData/Local/spark/spark-2.2.0-bin-hadoop2.7/tmp/hive/tehno-gate/89b35f50-56fc-446d-b56e-5e04534c0b33
18/06/29 03:28:01 INFO SessionState: Created local directory: C:/Users/tehno-gate/AppData/Local/spark/spark-2.2.0-bin-hadoop2.7/tmp/hive/89b35f50-56fc-446d-b56e-5e04534c0b33
18/06/29 03:28:01 INFO SessionState: Created HDFS directory: C:/Users/tehno-gate/AppData/Local/spark/spark-2.2.0-bin-hadoop2.7/tmp/hive/tehno-gate/89b35f50-56fc-446d-b56e-5e04534c0b33/_tmp_space.db
18/06/29 03:28:01 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is C:Users	ehno-gateAppDataLocalsparkspark-2.2.0-bin-hadoop2.7	mphive
18/06/29 03:28:01 INFO HiveMetaStore: 0: get_database: default
18/06/29 03:28:01 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: default	
18/06/29 03:28:01 INFO HiveMetaStore: 0: get_database: global_temp
18/06/29 03:28:01 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: global_temp	
18/06/29 03:28:01 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
18/06/29 03:28:01 INFO SessionState: Created local directory: C:/Users/TEHNO-~1/AppData/Local/Temp/51242e18-b1de-4c75-952a-3051dc0dd180_resources
18/06/29 03:28:01 INFO SessionState: Created HDFS directory: C:/Users/tehno-gate/AppData/Local/spark/spark-2.2.0-bin-hadoop2.7/tmp/hive/tehno-gate/51242e18-b1de-4c75-952a-3051dc0dd180
18/06/29 03:28:01 INFO SessionState: Created local directory: C:/Users/tehno-gate/AppData/Local/spark/spark-2.2.0-bin-hadoop2.7/tmp/hive/51242e18-b1de-4c75-952a-3051dc0dd180
18/06/29 03:28:01 INFO SessionState: Created HDFS directory: C:/Users/tehno-gate/AppData/Local/spark/spark-2.2.0-bin-hadoop2.7/tmp/hive/tehno-gate/51242e18-b1de-4c75-952a-3051dc0dd180/_tmp_space.db
18/06/29 03:28:01 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is C:Users	ehno-gateAppDataLocalsparkspark-2.2.0-bin-hadoop2.7	mphive
18/06/29 03:28:01 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
18/06/29 03:28:01 INFO SparkSqlParser: Parsing command: SHOW TABLES
18/06/29 03:28:04 INFO HiveMetaStore: 0: get_database: default
18/06/29 03:28:04 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: default	
18/06/29 03:28:04 INFO HiveMetaStore: 0: get_database: default
18/06/29 03:28:04 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: default	
18/06/29 03:28:04 INFO HiveMetaStore: 0: get_tables: db=default pat=*
18/06/29 03:28:04 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
18/06/29 03:28:06 INFO SparkContext: Starting job: collect at utils.scala:58
18/06/29 03:28:06 INFO DAGScheduler: Got job 0 (collect at utils.scala:58) with 1 output partitions
18/06/29 03:28:06 INFO DAGScheduler: Final stage: ResultStage 0 (collect at utils.scala:58)
18/06/29 03:28:06 INFO DAGScheduler: Parents of final stage: List()
18/06/29 03:28:06 INFO DAGScheduler: Missing parents: List()
18/06/29 03:28:06 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[4] at map at utils.scala:55), which has no missing parents
18/06/29 03:28:06 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 6.0 KB, free 366.3 MB)
18/06/29 03:28:06 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.4 KB, free 366.3 MB)
18/06/29 03:28:06 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 127.0.0.1:55643 (size: 3.4 KB, free: 366.3 MB)
18/06/29 03:28:06 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
18/06/29 03:28:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at map at utils.scala:55) (first 15 tasks are for partitions Vector(0))
18/06/29 03:28:06 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
18/06/29 03:28:06 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4852 bytes)
18/06/29 03:28:06 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
18/06/29 03:28:06 INFO Executor: Fetching spark://127.0.0.1:55622/jars/sparklyr-2.2-2.11.jar with timestamp 1530232070629
18/06/29 03:28:06 INFO TransportClientFactory: Successfully created connection to /127.0.0.1:55622 after 27 ms (0 ms spent in bootstraps)
18/06/29 03:28:06 INFO Utils: Fetching spark://127.0.0.1:55622/jars/sparklyr-2.2-2.11.jar to C:\Users\tehno-gate\AppData\Local\Temp\spark-32199f75-25dc-4509-8cd4-7379c01856d7\userFiles-5389b81c-5a98-4e7e-816f-7e60a4d69ae7\fetchFileTemp5990189741395520809.tmp
18/06/29 03:28:06 INFO Executor: Adding file:/C:/Users/tehno-gate/AppData/Local/Temp/spark-32199f75-25dc-4509-8cd4-7379c01856d7/userFiles-5389b81c-5a98-4e7e-816f-7e60a4d69ae7/sparklyr-2.2-2.11.jar to class loader
18/06/29 03:28:09 INFO CodeGenerator: Code generated in 341.743732 ms
18/06/29 03:28:09 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1013 bytes result sent to driver
18/06/29 03:28:09 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3059 ms on localhost (executor driver) (1/1)
18/06/29 03:28:09 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
18/06/29 03:28:09 INFO DAGScheduler: ResultStage 0 (collect at utils.scala:58) finished in 3.124 s
18/06/29 03:28:09 INFO DAGScheduler: Job 0 finished: collect at utils.scala:58, took 3.553226 s
18/06/29 03:28:18 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 127.0.0.1:55643 in memory (size: 3.4 KB, free: 366.3 MB)
18/06/29 03:28:18 INFO ContextCleaner: Cleaned accumulator 0
18/06/29 03:28:18 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 03:28:18 INFO SparkSqlParser: Parsing command: flights
18/06/29 03:28:18 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 03:28:18 INFO SparkSqlParser: Parsing command: CACHE TABLE `flights`
18/06/29 03:28:18 INFO SparkSqlParser: Parsing command: `flights`
18/06/29 03:28:18 INFO CodeGenerator: Code generated in 28.835392 ms
18/06/29 03:28:18 INFO CodeGenerator: Code generated in 20.07363 ms
18/06/29 03:28:18 INFO SparkContext: Starting job: sql at NativeMethodAccessorImpl.java:0
18/06/29 03:28:18 INFO DAGScheduler: Registering RDD 12 (sql at NativeMethodAccessorImpl.java:0)
18/06/29 03:28:18 INFO DAGScheduler: Got job 1 (sql at NativeMethodAccessorImpl.java:0) with 1 output partitions
18/06/29 03:28:18 INFO DAGScheduler: Final stage: ResultStage 2 (sql at NativeMethodAccessorImpl.java:0)
18/06/29 03:28:18 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
18/06/29 03:28:18 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)
18/06/29 03:28:18 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[12] at sql at NativeMethodAccessorImpl.java:0), which has no missing parents
18/06/29 03:28:18 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 31.1 KB, free 366.3 MB)
18/06/29 03:28:18 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 11.6 KB, free 366.3 MB)
18/06/29 03:28:18 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 127.0.0.1:55643 (size: 11.6 KB, free: 366.3 MB)
18/06/29 03:28:18 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
18/06/29 03:28:18 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[12] at sql at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
18/06/29 03:28:18 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
18/06/29 03:28:19 INFO ContextCleaner: Cleaned accumulator 51
18/06/29 03:28:19 WARN TaskSetManager: Stage 1 contains a task of very large size (32853 KB). The maximum recommended task size is 100 KB.
18/06/29 03:28:19 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 33641611 bytes)
18/06/29 03:28:19 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
18/06/29 03:28:20 INFO CodeGenerator: Code generated in 39.710168 ms
18/06/29 03:28:20 INFO CodeGenerator: Code generated in 216.124468 ms
18/06/29 03:28:25 INFO MemoryStore: Block rdd_9_0 stored as values in memory (estimated size 10.4 MB, free 355.9 MB)
18/06/29 03:28:25 INFO BlockManagerInfo: Added rdd_9_0 in memory on 127.0.0.1:55643 (size: 10.4 MB, free: 355.9 MB)
18/06/29 03:28:26 INFO CodeGenerator: Code generated in 7.793398 ms
18/06/29 03:28:26 INFO CodeGenerator: Code generated in 32.719714 ms
18/06/29 03:28:26 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2371 bytes result sent to driver
18/06/29 03:28:26 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 7390 ms on localhost (executor driver) (1/1)
18/06/29 03:28:26 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
18/06/29 03:28:26 INFO DAGScheduler: ShuffleMapStage 1 (sql at NativeMethodAccessorImpl.java:0) finished in 7.393 s
18/06/29 03:28:26 INFO DAGScheduler: looking for newly runnable stages
18/06/29 03:28:26 INFO DAGScheduler: running: Set()
18/06/29 03:28:26 INFO DAGScheduler: waiting: Set(ResultStage 2)
18/06/29 03:28:26 INFO DAGScheduler: failed: Set()
18/06/29 03:28:26 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[15] at sql at NativeMethodAccessorImpl.java:0), which has no missing parents
18/06/29 03:28:26 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 7.0 KB, free 355.9 MB)
18/06/29 03:28:26 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.7 KB, free 355.9 MB)
18/06/29 03:28:26 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 127.0.0.1:55643 (size: 3.7 KB, free: 355.9 MB)
18/06/29 03:28:26 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
18/06/29 03:28:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[15] at sql at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
18/06/29 03:28:26 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
18/06/29 03:28:26 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 4726 bytes)
18/06/29 03:28:26 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
18/06/29 03:28:26 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
18/06/29 03:28:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
18/06/29 03:28:26 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1538 bytes result sent to driver
18/06/29 03:28:26 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 63 ms on localhost (executor driver) (1/1)
18/06/29 03:28:26 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
18/06/29 03:28:26 INFO DAGScheduler: ResultStage 2 (sql at NativeMethodAccessorImpl.java:0) finished in 0.064 s
18/06/29 03:28:26 INFO DAGScheduler: Job 1 finished: sql at NativeMethodAccessorImpl.java:0, took 7.539593 s
18/06/29 03:28:26 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 03:28:26 INFO SparkSqlParser: Parsing command: SELECT count(*) FROM  `flights`
18/06/29 03:28:26 INFO HiveMetaStore: 0: get_database: default
18/06/29 03:28:26 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: default	
18/06/29 03:28:26 INFO SparkContext: Starting job: collect at utils.scala:211
18/06/29 03:28:26 INFO DAGScheduler: Registering RDD 18 (collect at utils.scala:211)
18/06/29 03:28:26 INFO DAGScheduler: Got job 2 (collect at utils.scala:211) with 1 output partitions
18/06/29 03:28:26 INFO DAGScheduler: Final stage: ResultStage 4 (collect at utils.scala:211)
18/06/29 03:28:26 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
18/06/29 03:28:26 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 3)
18/06/29 03:28:26 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[18] at collect at utils.scala:211), which has no missing parents
18/06/29 03:28:26 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 31.1 KB, free 355.8 MB)
18/06/29 03:28:26 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 11.5 KB, free 355.8 MB)
18/06/29 03:28:26 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 127.0.0.1:55643 (size: 11.5 KB, free: 355.9 MB)
18/06/29 03:28:26 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1006
18/06/29 03:28:26 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[18] at collect at utils.scala:211) (first 15 tasks are for partitions Vector(0))
18/06/29 03:28:26 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
18/06/29 03:28:26 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 127.0.0.1:55643 in memory (size: 3.7 KB, free: 355.9 MB)
18/06/29 03:28:26 INFO ContextCleaner: Cleaned accumulator 112
18/06/29 03:28:26 WARN TaskSetManager: Stage 3 contains a task of very large size (32853 KB). The maximum recommended task size is 100 KB.
18/06/29 03:28:26 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 33641611 bytes)
18/06/29 03:28:26 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
18/06/29 03:28:27 INFO BlockManager: Found block rdd_9_0 locally
18/06/29 03:28:27 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1776 bytes result sent to driver
18/06/29 03:28:27 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 800 ms on localhost (executor driver) (1/1)
18/06/29 03:28:27 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
18/06/29 03:28:27 INFO DAGScheduler: ShuffleMapStage 3 (collect at utils.scala:211) finished in 0.802 s
18/06/29 03:28:27 INFO DAGScheduler: looking for newly runnable stages
18/06/29 03:28:27 INFO DAGScheduler: running: Set()
18/06/29 03:28:27 INFO DAGScheduler: waiting: Set(ResultStage 4)
18/06/29 03:28:27 INFO DAGScheduler: failed: Set()
18/06/29 03:28:27 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[21] at collect at utils.scala:211), which has no missing parents
18/06/29 03:28:27 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 7.0 KB, free 355.8 MB)
18/06/29 03:28:27 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.7 KB, free 355.8 MB)
18/06/29 03:28:27 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 127.0.0.1:55643 (size: 3.7 KB, free: 355.9 MB)
18/06/29 03:28:27 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1006
18/06/29 03:28:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[21] at collect at utils.scala:211) (first 15 tasks are for partitions Vector(0))
18/06/29 03:28:27 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
18/06/29 03:28:27 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, ANY, 4726 bytes)
18/06/29 03:28:27 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
18/06/29 03:28:27 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
18/06/29 03:28:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
18/06/29 03:28:27 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1495 bytes result sent to driver
18/06/29 03:28:27 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 9 ms on localhost (executor driver) (1/1)
18/06/29 03:28:27 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
18/06/29 03:28:27 INFO DAGScheduler: ResultStage 4 (collect at utils.scala:211) finished in 0.010 s
18/06/29 03:28:27 INFO DAGScheduler: Job 2 finished: collect at utils.scala:211, took 0.851191 s
18/06/29 03:28:27 INFO CodeGenerator: Code generated in 9.397477 ms
18/06/29 03:28:27 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 03:28:27 INFO SparkSqlParser: Parsing command: SELECT *
FROM `flights` AS `zzz1`
WHERE (0 = 1)
18/06/29 03:28:28 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 03:28:28 INFO SparkSqlParser: Parsing command: SELECT *
FROM `flights`
18/06/29 03:28:29 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 127.0.0.1:55643 in memory (size: 3.7 KB, free: 355.9 MB)
18/06/29 03:28:29 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 127.0.0.1:55643 in memory (size: 11.5 KB, free: 355.9 MB)
18/06/29 03:28:29 INFO SparkSqlParser: Parsing command: sparklyr_tmp_1d582f8f3f28
18/06/29 03:28:29 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 03:28:29 INFO SparkSqlParser: Parsing command: SELECT *
FROM `sparklyr_tmp_1d582f8f3f28` AS `zzz2`
WHERE (0 = 1)
18/06/29 03:28:29 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 03:28:29 INFO SparkSqlParser: Parsing command: SELECT *
FROM `sparklyr_tmp_1d582f8f3f28`
18/06/29 03:28:29 INFO CodeGenerator: Code generated in 50.126117 ms
18/06/29 03:28:29 INFO SparkContext: Starting job: collect at utils.scala:211
18/06/29 03:28:29 INFO DAGScheduler: Got job 3 (collect at utils.scala:211) with 1 output partitions
18/06/29 03:28:29 INFO DAGScheduler: Final stage: ResultStage 5 (collect at utils.scala:211)
18/06/29 03:28:29 INFO DAGScheduler: Parents of final stage: List()
18/06/29 03:28:29 INFO DAGScheduler: Missing parents: List()
18/06/29 03:28:29 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[24] at collect at utils.scala:211), which has no missing parents
18/06/29 03:28:29 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 40.7 KB, free 355.8 MB)
18/06/29 03:28:29 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 14.9 KB, free 355.8 MB)
18/06/29 03:28:29 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 127.0.0.1:55643 (size: 14.9 KB, free: 355.9 MB)
18/06/29 03:28:29 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1006
18/06/29 03:28:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[24] at collect at utils.scala:211) (first 15 tasks are for partitions Vector(0))
18/06/29 03:28:29 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks
18/06/29 03:28:30 WARN TaskSetManager: Stage 5 contains a task of very large size (32853 KB). The maximum recommended task size is 100 KB.
18/06/29 03:28:30 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 33641622 bytes)
18/06/29 03:28:30 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
18/06/29 03:28:30 INFO ContextCleaner: Cleaned accumulator 53
18/06/29 03:28:30 INFO ContextCleaner: Cleaned accumulator 52
18/06/29 03:28:30 INFO ContextCleaner: Cleaned accumulator 62
18/06/29 03:28:30 INFO ContextCleaner: Cleaned accumulator 55
18/06/29 03:28:30 INFO ContextCleaner: Cleaned accumulator 63
18/06/29 03:28:30 INFO ContextCleaner: Cleaned accumulator 61
18/06/29 03:28:30 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 127.0.0.1:55643 in memory (size: 11.6 KB, free: 355.9 MB)
18/06/29 03:28:30 INFO ContextCleaner: Cleaned accumulator 56
18/06/29 03:28:30 INFO ContextCleaner: Cleaned shuffle 0
18/06/29 03:28:30 INFO ContextCleaner: Cleaned accumulator 60
18/06/29 03:28:30 INFO ContextCleaner: Cleaned accumulator 59
18/06/29 03:28:30 INFO ContextCleaner: Cleaned accumulator 54
18/06/29 03:28:30 INFO ContextCleaner: Cleaned accumulator 57
18/06/29 03:28:30 INFO ContextCleaner: Cleaned accumulator 58
18/06/29 03:28:30 INFO BlockManager: Found block rdd_9_0 locally
18/06/29 03:28:31 INFO CodeGenerator: Code generated in 106.699944 ms
18/06/29 03:28:32 INFO MemoryStore: Block taskresult_5 stored as bytes in memory (estimated size 22.4 MB, free 333.4 MB)
18/06/29 03:28:32 INFO BlockManagerInfo: Added taskresult_5 in memory on 127.0.0.1:55643 (size: 22.4 MB, free: 333.5 MB)
18/06/29 03:28:32 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 23505630 bytes result sent via BlockManager)
18/06/29 03:28:32 INFO TransportClientFactory: Successfully created connection to /127.0.0.1:55643 after 5 ms (0 ms spent in bootstraps)
18/06/29 03:28:33 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 3227 ms on localhost (executor driver) (1/1)
18/06/29 03:28:33 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
18/06/29 03:28:33 INFO DAGScheduler: ResultStage 5 (collect at utils.scala:211) finished in 3.230 s
18/06/29 03:28:33 INFO DAGScheduler: Job 3 finished: collect at utils.scala:211, took 3.248952 s
18/06/29 03:28:33 INFO BlockManagerInfo: Removed taskresult_5 on 127.0.0.1:55643 in memory (size: 22.4 MB, free: 355.9 MB)
18/06/29 03:28:33 INFO CodeGenerator: Code generated in 25.993086 ms
18/06/29 03:28:34 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 127.0.0.1:55643 in memory (size: 14.9 KB, free: 355.9 MB)
18/06/29 03:28:40 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 03:28:40 INFO SparkSqlParser: Parsing command: SHOW TABLES
18/06/29 03:28:40 INFO HiveMetaStore: 0: get_database: default
18/06/29 03:28:40 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: default	
18/06/29 03:28:40 INFO HiveMetaStore: 0: get_database: default
18/06/29 03:28:40 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: default	
18/06/29 03:28:40 INFO HiveMetaStore: 0: get_tables: db=default pat=*
18/06/29 03:28:40 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
18/06/29 03:28:40 INFO CodeGenerator: Code generated in 12.330944 ms
18/06/29 03:28:40 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 03:28:40 INFO SparkSqlParser: Parsing command: SHOW TABLES
18/06/29 03:28:40 INFO HiveMetaStore: 0: get_database: default
18/06/29 03:28:40 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: default	
18/06/29 03:28:40 INFO HiveMetaStore: 0: get_database: default
18/06/29 03:28:40 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: default	
18/06/29 03:28:40 INFO HiveMetaStore: 0: get_tables: db=default pat=*
18/06/29 03:28:40 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
18/06/29 03:41:23 INFO SparkContext: Running Spark version 2.2.0
18/06/29 03:41:24 INFO SparkContext: Submitted application: sparklyr
18/06/29 03:41:24 INFO SecurityManager: Changing view acls to: tehno-gate
18/06/29 03:41:24 INFO SecurityManager: Changing modify acls to: tehno-gate
18/06/29 03:41:24 INFO SecurityManager: Changing view acls groups to: 
18/06/29 03:41:24 INFO SecurityManager: Changing modify acls groups to: 
18/06/29 03:41:24 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tehno-gate); groups with view permissions: Set(); users  with modify permissions: Set(tehno-gate); groups with modify permissions: Set()
18/06/29 03:41:25 INFO Utils: Successfully started service 'sparkDriver' on port 55864.
18/06/29 03:41:25 INFO SparkEnv: Registering MapOutputTracker
18/06/29 03:41:25 INFO SparkEnv: Registering BlockManagerMaster
18/06/29 03:41:25 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
18/06/29 03:41:25 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
18/06/29 03:41:25 INFO DiskBlockManager: Created local directory at C:\Users\tehno-gate\AppData\Local\Temp\blockmgr-f8efe915-61ea-495b-876f-5bb30f3c32f4
18/06/29 03:41:25 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
18/06/29 03:41:25 INFO SparkEnv: Registering OutputCommitCoordinator
18/06/29 03:41:25 INFO Utils: Successfully started service 'SparkUI' on port 4040.
18/06/29 03:41:26 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://127.0.0.1:4040
18/06/29 03:41:26 INFO SparkContext: Added JAR file:/C:/Users/tehno-gate/Documents/R/win-library/3.4/sparklyr/java/sparklyr-2.2-2.11.jar at spark://127.0.0.1:55864/jars/sparklyr-2.2-2.11.jar with timestamp 1530232886074
18/06/29 03:41:26 INFO Executor: Starting executor ID driver on host localhost
18/06/29 03:41:26 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55885.
18/06/29 03:41:26 INFO NettyBlockTransferService: Server created on 127.0.0.1:55885
18/06/29 03:41:26 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
18/06/29 03:41:26 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 55885, None)
18/06/29 03:41:26 INFO BlockManagerMasterEndpoint: Registering block manager 127.0.0.1:55885 with 366.3 MB RAM, BlockManagerId(driver, 127.0.0.1, 55885, None)
18/06/29 03:41:26 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 55885, None)
18/06/29 03:41:26 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 127.0.0.1, 55885, None)
18/06/29 03:41:27 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.
18/06/29 03:41:27 INFO SharedState: loading hive config file: file:/C:/Users/tehno-gate/AppData/Local/spark/spark-2.2.0-bin-hadoop2.7/conf/hive-site.xml
18/06/29 03:41:27 INFO SharedState: Setting hive.metastore.warehouse.dir ('C:\Users\tehno-gate\AppData\Local\spark\spark-2.2.0-bin-hadoop2.7\tmp\hive') to the value of spark.sql.warehouse.dir ('C:Users	ehno-gateAppDataLocalsparkspark-2.2.0-bin-hadoop2.7	mphive').
18/06/29 03:41:27 INFO SharedState: Warehouse path is 'C:Users	ehno-gateAppDataLocalsparkspark-2.2.0-bin-hadoop2.7	mphive'.
18/06/29 03:41:29 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
18/06/29 03:41:30 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
18/06/29 03:41:30 INFO ObjectStore: ObjectStore, initialize called
18/06/29 03:41:30 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
18/06/29 03:41:30 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
18/06/29 03:41:33 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
18/06/29 03:41:34 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
18/06/29 03:41:34 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
18/06/29 03:41:35 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
18/06/29 03:41:35 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
18/06/29 03:41:36 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
18/06/29 03:41:36 INFO ObjectStore: Initialized ObjectStore
18/06/29 03:41:36 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
18/06/29 03:41:36 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
18/06/29 03:41:36 INFO HiveMetaStore: Added admin role in metastore
18/06/29 03:41:36 INFO HiveMetaStore: Added public role in metastore
18/06/29 03:41:36 INFO HiveMetaStore: No user is added in admin role, since config is empty
18/06/29 03:41:36 INFO HiveMetaStore: 0: get_all_databases
18/06/29 03:41:36 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_all_databases	
18/06/29 03:41:36 INFO HiveMetaStore: 0: get_functions: db=default pat=*
18/06/29 03:41:36 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
18/06/29 03:41:36 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
18/06/29 03:41:37 INFO SessionState: Created local directory: C:/Users/TEHNO-~1/AppData/Local/Temp/356b1112-75de-4651-b376-45b2cbe365ee_resources
18/06/29 03:41:37 INFO SessionState: Created HDFS directory: C:/Users/tehno-gate/AppData/Local/spark/spark-2.2.0-bin-hadoop2.7/tmp/hive/tehno-gate/356b1112-75de-4651-b376-45b2cbe365ee
18/06/29 03:41:37 INFO SessionState: Created local directory: C:/Users/tehno-gate/AppData/Local/spark/spark-2.2.0-bin-hadoop2.7/tmp/hive/356b1112-75de-4651-b376-45b2cbe365ee
18/06/29 03:41:37 INFO SessionState: Created HDFS directory: C:/Users/tehno-gate/AppData/Local/spark/spark-2.2.0-bin-hadoop2.7/tmp/hive/tehno-gate/356b1112-75de-4651-b376-45b2cbe365ee/_tmp_space.db
18/06/29 03:41:37 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is C:Users	ehno-gateAppDataLocalsparkspark-2.2.0-bin-hadoop2.7	mphive
18/06/29 03:41:37 INFO HiveMetaStore: 0: get_database: default
18/06/29 03:41:37 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: default	
18/06/29 03:41:37 INFO HiveMetaStore: 0: get_database: global_temp
18/06/29 03:41:37 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: global_temp	
18/06/29 03:41:37 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
18/06/29 03:41:37 INFO SessionState: Created local directory: C:/Users/TEHNO-~1/AppData/Local/Temp/4de5d2a2-3fa2-45ab-98ad-d56349919653_resources
18/06/29 03:41:37 INFO SessionState: Created HDFS directory: C:/Users/tehno-gate/AppData/Local/spark/spark-2.2.0-bin-hadoop2.7/tmp/hive/tehno-gate/4de5d2a2-3fa2-45ab-98ad-d56349919653
18/06/29 03:41:37 INFO SessionState: Created local directory: C:/Users/tehno-gate/AppData/Local/spark/spark-2.2.0-bin-hadoop2.7/tmp/hive/4de5d2a2-3fa2-45ab-98ad-d56349919653
18/06/29 03:41:37 INFO SessionState: Created HDFS directory: C:/Users/tehno-gate/AppData/Local/spark/spark-2.2.0-bin-hadoop2.7/tmp/hive/tehno-gate/4de5d2a2-3fa2-45ab-98ad-d56349919653/_tmp_space.db
18/06/29 03:41:37 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is C:Users	ehno-gateAppDataLocalsparkspark-2.2.0-bin-hadoop2.7	mphive
18/06/29 03:41:37 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
18/06/29 03:41:37 INFO SparkSqlParser: Parsing command: SHOW TABLES
18/06/29 03:41:41 INFO HiveMetaStore: 0: get_database: default
18/06/29 03:41:41 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: default	
18/06/29 03:41:41 INFO HiveMetaStore: 0: get_database: default
18/06/29 03:41:41 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: default	
18/06/29 03:41:41 INFO HiveMetaStore: 0: get_tables: db=default pat=*
18/06/29 03:41:41 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
18/06/29 03:42:23 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 03:42:23 INFO SparkSqlParser: Parsing command: SHOW TABLES
18/06/29 03:42:23 INFO HiveMetaStore: 0: get_database: default
18/06/29 03:42:23 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: default	
18/06/29 03:42:23 INFO HiveMetaStore: 0: get_database: default
18/06/29 03:42:23 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: default	
18/06/29 03:42:23 INFO HiveMetaStore: 0: get_tables: db=default pat=*
18/06/29 03:42:23 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
18/06/29 03:42:24 INFO SparkContext: Starting job: collect at utils.scala:58
18/06/29 03:42:24 INFO DAGScheduler: Got job 0 (collect at utils.scala:58) with 1 output partitions
18/06/29 03:42:24 INFO DAGScheduler: Final stage: ResultStage 0 (collect at utils.scala:58)
18/06/29 03:42:24 INFO DAGScheduler: Parents of final stage: List()
18/06/29 03:42:24 INFO DAGScheduler: Missing parents: List()
18/06/29 03:42:24 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[4] at map at utils.scala:55), which has no missing parents
18/06/29 03:42:24 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 6.0 KB, free 366.3 MB)
18/06/29 03:42:24 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.4 KB, free 366.3 MB)
18/06/29 03:42:24 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 127.0.0.1:55885 (size: 3.4 KB, free: 366.3 MB)
18/06/29 03:42:24 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
18/06/29 03:42:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at map at utils.scala:55) (first 15 tasks are for partitions Vector(0))
18/06/29 03:42:24 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
18/06/29 03:42:24 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4852 bytes)
18/06/29 03:42:24 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
18/06/29 03:42:24 INFO Executor: Fetching spark://127.0.0.1:55864/jars/sparklyr-2.2-2.11.jar with timestamp 1530232886074
18/06/29 03:42:24 INFO TransportClientFactory: Successfully created connection to /127.0.0.1:55864 after 26 ms (0 ms spent in bootstraps)
18/06/29 03:42:24 INFO Utils: Fetching spark://127.0.0.1:55864/jars/sparklyr-2.2-2.11.jar to C:\Users\tehno-gate\AppData\Local\Temp\spark-415ebe47-7156-4ff7-9806-4da5e1159b5e\userFiles-8c0061d7-2923-4799-890b-5542fc4b02a9\fetchFileTemp1831879677893532320.tmp
18/06/29 03:42:24 INFO Executor: Adding file:/C:/Users/tehno-gate/AppData/Local/Temp/spark-415ebe47-7156-4ff7-9806-4da5e1159b5e/userFiles-8c0061d7-2923-4799-890b-5542fc4b02a9/sparklyr-2.2-2.11.jar to class loader
18/06/29 03:42:25 INFO CodeGenerator: Code generated in 362.789951 ms
18/06/29 03:42:25 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1013 bytes result sent to driver
18/06/29 03:42:25 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1145 ms on localhost (executor driver) (1/1)
18/06/29 03:42:25 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
18/06/29 03:42:25 INFO DAGScheduler: ResultStage 0 (collect at utils.scala:58) finished in 1.194 s
18/06/29 03:42:25 INFO DAGScheduler: Job 0 finished: collect at utils.scala:58, took 1.505560 s
18/06/29 03:42:40 INFO ContextCleaner: Cleaned accumulator 0
18/06/29 03:42:41 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 127.0.0.1:55885 in memory (size: 3.4 KB, free: 366.3 MB)
18/06/29 03:42:41 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 03:42:41 INFO SparkSqlParser: Parsing command: flights
18/06/29 03:42:41 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 03:42:41 INFO SparkSqlParser: Parsing command: CACHE TABLE `flights`
18/06/29 03:42:41 INFO SparkSqlParser: Parsing command: `flights`
18/06/29 03:42:41 INFO CodeGenerator: Code generated in 24.282752 ms
18/06/29 03:42:41 INFO CodeGenerator: Code generated in 18.85593 ms
18/06/29 03:42:41 INFO SparkContext: Starting job: sql at NativeMethodAccessorImpl.java:0
18/06/29 03:42:41 INFO DAGScheduler: Registering RDD 12 (sql at NativeMethodAccessorImpl.java:0)
18/06/29 03:42:41 INFO DAGScheduler: Got job 1 (sql at NativeMethodAccessorImpl.java:0) with 1 output partitions
18/06/29 03:42:41 INFO DAGScheduler: Final stage: ResultStage 2 (sql at NativeMethodAccessorImpl.java:0)
18/06/29 03:42:41 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
18/06/29 03:42:41 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)
18/06/29 03:42:41 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[12] at sql at NativeMethodAccessorImpl.java:0), which has no missing parents
18/06/29 03:42:41 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 30.7 KB, free 366.3 MB)
18/06/29 03:42:41 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 11.8 KB, free 366.3 MB)
18/06/29 03:42:41 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 127.0.0.1:55885 (size: 11.8 KB, free: 366.3 MB)
18/06/29 03:42:41 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
18/06/29 03:42:41 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[12] at sql at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
18/06/29 03:42:41 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
18/06/29 03:42:42 INFO ContextCleaner: Cleaned accumulator 51
18/06/29 03:42:42 WARN TaskSetManager: Stage 1 contains a task of very large size (27699 KB). The maximum recommended task size is 100 KB.
18/06/29 03:42:42 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 28364103 bytes)
18/06/29 03:42:42 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
18/06/29 03:42:42 INFO CodeGenerator: Code generated in 43.298668 ms
18/06/29 03:42:43 INFO CodeGenerator: Code generated in 254.057892 ms
18/06/29 03:42:48 INFO MemoryStore: Block rdd_9_0 stored as values in memory (estimated size 22.5 MB, free 343.8 MB)
18/06/29 03:42:48 INFO BlockManagerInfo: Added rdd_9_0 in memory on 127.0.0.1:55885 (size: 22.5 MB, free: 343.8 MB)
18/06/29 03:42:48 INFO CodeGenerator: Code generated in 8.508803 ms
18/06/29 03:42:48 INFO CodeGenerator: Code generated in 24.753049 ms
18/06/29 03:42:48 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2371 bytes result sent to driver
18/06/29 03:42:48 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 7228 ms on localhost (executor driver) (1/1)
18/06/29 03:42:48 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
18/06/29 03:42:48 INFO DAGScheduler: ShuffleMapStage 1 (sql at NativeMethodAccessorImpl.java:0) finished in 7.231 s
18/06/29 03:42:48 INFO DAGScheduler: looking for newly runnable stages
18/06/29 03:42:48 INFO DAGScheduler: running: Set()
18/06/29 03:42:48 INFO DAGScheduler: waiting: Set(ResultStage 2)
18/06/29 03:42:48 INFO DAGScheduler: failed: Set()
18/06/29 03:42:48 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[15] at sql at NativeMethodAccessorImpl.java:0), which has no missing parents
18/06/29 03:42:48 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 7.0 KB, free 343.8 MB)
18/06/29 03:42:48 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.7 KB, free 343.8 MB)
18/06/29 03:42:49 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 127.0.0.1:55885 (size: 3.7 KB, free: 343.8 MB)
18/06/29 03:42:49 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
18/06/29 03:42:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[15] at sql at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
18/06/29 03:42:49 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
18/06/29 03:42:49 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 4726 bytes)
18/06/29 03:42:49 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
18/06/29 03:42:49 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
18/06/29 03:42:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
18/06/29 03:42:49 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1624 bytes result sent to driver
18/06/29 03:42:49 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 61 ms on localhost (executor driver) (1/1)
18/06/29 03:42:49 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
18/06/29 03:42:49 INFO DAGScheduler: ResultStage 2 (sql at NativeMethodAccessorImpl.java:0) finished in 0.063 s
18/06/29 03:42:49 INFO DAGScheduler: Job 1 finished: sql at NativeMethodAccessorImpl.java:0, took 7.389127 s
18/06/29 03:42:49 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 03:42:49 INFO SparkSqlParser: Parsing command: SELECT count(*) FROM  `flights`
18/06/29 03:42:49 INFO HiveMetaStore: 0: get_database: default
18/06/29 03:42:49 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: default	
18/06/29 03:42:49 INFO SparkContext: Starting job: collect at utils.scala:211
18/06/29 03:42:49 INFO DAGScheduler: Registering RDD 18 (collect at utils.scala:211)
18/06/29 03:42:49 INFO DAGScheduler: Got job 2 (collect at utils.scala:211) with 1 output partitions
18/06/29 03:42:49 INFO DAGScheduler: Final stage: ResultStage 4 (collect at utils.scala:211)
18/06/29 03:42:49 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
18/06/29 03:42:49 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 3)
18/06/29 03:42:49 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[18] at collect at utils.scala:211), which has no missing parents
18/06/29 03:42:49 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 30.7 KB, free 343.7 MB)
18/06/29 03:42:49 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 11.7 KB, free 343.7 MB)
18/06/29 03:42:49 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 127.0.0.1:55885 (size: 11.7 KB, free: 343.8 MB)
18/06/29 03:42:49 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1006
18/06/29 03:42:49 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[18] at collect at utils.scala:211) (first 15 tasks are for partitions Vector(0))
18/06/29 03:42:49 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
18/06/29 03:42:49 INFO ContextCleaner: Cleaned accumulator 58
18/06/29 03:42:49 INFO ContextCleaner: Cleaned accumulator 112
18/06/29 03:42:49 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 127.0.0.1:55885 in memory (size: 3.7 KB, free: 343.8 MB)
18/06/29 03:42:49 INFO ContextCleaner: Cleaned accumulator 61
18/06/29 03:42:49 INFO ContextCleaner: Cleaned accumulator 60
18/06/29 03:42:49 INFO ContextCleaner: Cleaned accumulator 56
18/06/29 03:42:49 INFO ContextCleaner: Cleaned accumulator 62
18/06/29 03:42:49 INFO ContextCleaner: Cleaned accumulator 55
18/06/29 03:42:49 INFO ContextCleaner: Cleaned accumulator 59
18/06/29 03:42:49 INFO ContextCleaner: Cleaned shuffle 0
18/06/29 03:42:49 INFO ContextCleaner: Cleaned accumulator 57
18/06/29 03:42:49 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 127.0.0.1:55885 in memory (size: 11.8 KB, free: 343.8 MB)
18/06/29 03:42:49 INFO ContextCleaner: Cleaned accumulator 52
18/06/29 03:42:49 INFO ContextCleaner: Cleaned accumulator 63
18/06/29 03:42:49 INFO ContextCleaner: Cleaned accumulator 53
18/06/29 03:42:49 INFO ContextCleaner: Cleaned accumulator 54
18/06/29 03:42:50 WARN TaskSetManager: Stage 3 contains a task of very large size (27699 KB). The maximum recommended task size is 100 KB.
18/06/29 03:42:50 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 28364103 bytes)
18/06/29 03:42:50 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
18/06/29 03:42:50 INFO BlockManager: Found block rdd_9_0 locally
18/06/29 03:42:50 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1690 bytes result sent to driver
18/06/29 03:42:50 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 1169 ms on localhost (executor driver) (1/1)
18/06/29 03:42:50 INFO DAGScheduler: ShuffleMapStage 3 (collect at utils.scala:211) finished in 1.171 s
18/06/29 03:42:50 INFO DAGScheduler: looking for newly runnable stages
18/06/29 03:42:50 INFO DAGScheduler: running: Set()
18/06/29 03:42:50 INFO DAGScheduler: waiting: Set(ResultStage 4)
18/06/29 03:42:50 INFO DAGScheduler: failed: Set()
18/06/29 03:42:50 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[21] at collect at utils.scala:211), which has no missing parents
18/06/29 03:42:50 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 7.0 KB, free 343.8 MB)
18/06/29 03:42:50 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.7 KB, free 343.8 MB)
18/06/29 03:42:50 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
18/06/29 03:42:50 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 127.0.0.1:55885 (size: 3.7 KB, free: 343.8 MB)
18/06/29 03:42:50 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1006
18/06/29 03:42:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[21] at collect at utils.scala:211) (first 15 tasks are for partitions Vector(0))
18/06/29 03:42:50 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
18/06/29 03:42:50 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, ANY, 4726 bytes)
18/06/29 03:42:50 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
18/06/29 03:42:50 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
18/06/29 03:42:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
18/06/29 03:42:50 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1581 bytes result sent to driver
18/06/29 03:42:50 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 11 ms on localhost (executor driver) (1/1)
18/06/29 03:42:50 INFO DAGScheduler: ResultStage 4 (collect at utils.scala:211) finished in 0.012 s
18/06/29 03:42:50 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
18/06/29 03:42:50 INFO DAGScheduler: Job 2 finished: collect at utils.scala:211, took 1.222222 s
18/06/29 03:42:50 INFO CodeGenerator: Code generated in 11.681343 ms
18/06/29 03:42:50 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 03:42:50 INFO SparkSqlParser: Parsing command: SELECT *
FROM `flights` AS `zzz1`
WHERE (0 = 1)
18/06/29 03:42:51 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 03:42:51 INFO SparkSqlParser: Parsing command: SHOW TABLES
18/06/29 03:42:51 INFO HiveMetaStore: 0: get_database: default
18/06/29 03:42:51 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: default	
18/06/29 03:42:51 INFO HiveMetaStore: 0: get_database: default
18/06/29 03:42:51 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_database: default	
18/06/29 03:42:51 INFO HiveMetaStore: 0: get_tables: db=default pat=*
18/06/29 03:42:51 INFO audit: ugi=tehno-gate	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
18/06/29 03:42:51 INFO CodeGenerator: Code generated in 14.027392 ms
18/06/29 03:42:54 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 03:42:54 INFO SparkSqlParser: Parsing command: SELECT *
FROM `flights`
18/06/29 03:42:55 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 127.0.0.1:55885 in memory (size: 3.7 KB, free: 343.8 MB)
18/06/29 03:42:55 INFO SparkSqlParser: Parsing command: sparklyr_tmp_13a8e145a13
18/06/29 03:42:55 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 03:42:55 INFO SparkSqlParser: Parsing command: SELECT *
FROM `sparklyr_tmp_13a8e145a13` AS `zzz2`
WHERE (0 = 1)
18/06/29 03:42:55 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
18/06/29 03:42:55 INFO SparkSqlParser: Parsing command: SELECT *
FROM `sparklyr_tmp_13a8e145a13`
18/06/29 03:42:55 INFO CodeGenerator: Code generated in 49.005012 ms
18/06/29 03:42:55 INFO SparkContext: Starting job: collect at utils.scala:211
18/06/29 03:42:55 INFO DAGScheduler: Got job 3 (collect at utils.scala:211) with 1 output partitions
18/06/29 03:42:55 INFO DAGScheduler: Final stage: ResultStage 5 (collect at utils.scala:211)
18/06/29 03:42:55 INFO DAGScheduler: Parents of final stage: List()
18/06/29 03:42:55 INFO DAGScheduler: Missing parents: List()
18/06/29 03:42:55 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[24] at collect at utils.scala:211), which has no missing parents
18/06/29 03:42:55 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 39.5 KB, free 343.7 MB)
18/06/29 03:42:55 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 14.8 KB, free 343.7 MB)
18/06/29 03:42:55 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 127.0.0.1:55885 (size: 14.8 KB, free: 343.8 MB)
18/06/29 03:42:55 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1006
18/06/29 03:42:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[24] at collect at utils.scala:211) (first 15 tasks are for partitions Vector(0))
18/06/29 03:42:55 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks
18/06/29 03:42:55 WARN TaskSetManager: Stage 5 contains a task of very large size (27699 KB). The maximum recommended task size is 100 KB.
18/06/29 03:42:55 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 28364114 bytes)
18/06/29 03:42:55 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
18/06/29 03:42:55 INFO BlockManager: Found block rdd_9_0 locally
18/06/29 03:42:56 INFO CodeGenerator: Code generated in 47.586878 ms
18/06/29 03:42:57 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 127.0.0.1:55885 in memory (size: 11.7 KB, free: 343.8 MB)
18/06/29 03:42:57 INFO MemoryStore: Block taskresult_5 stored as bytes in memory (estimated size 19.2 MB, free 324.6 MB)
18/06/29 03:42:57 INFO BlockManagerInfo: Added taskresult_5 in memory on 127.0.0.1:55885 (size: 19.2 MB, free: 324.6 MB)
18/06/29 03:42:57 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 20124039 bytes result sent via BlockManager)
18/06/29 03:42:57 INFO TransportClientFactory: Successfully created connection to /127.0.0.1:55885 after 3 ms (0 ms spent in bootstraps)
18/06/29 03:42:58 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 2553 ms on localhost (executor driver) (1/1)
18/06/29 03:42:58 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
18/06/29 03:42:58 INFO DAGScheduler: ResultStage 5 (collect at utils.scala:211) finished in 2.555 s
18/06/29 03:42:58 INFO DAGScheduler: Job 3 finished: collect at utils.scala:211, took 2.572726 s
18/06/29 03:42:58 INFO BlockManagerInfo: Removed taskresult_5 on 127.0.0.1:55885 in memory (size: 19.2 MB, free: 343.8 MB)
18/06/29 03:42:58 INFO CodeGenerator: Code generated in 36.867258 ms
18/06/29 03:42:59 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 127.0.0.1:55885 in memory (size: 14.8 KB, free: 343.8 MB)
18/06/29 03:51:39 INFO SparkContext: Invoking stop() from shutdown hook
18/06/29 03:51:39 INFO SparkUI: Stopped Spark web UI at http://127.0.0.1:4040
18/06/29 03:51:39 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
18/06/29 03:51:39 INFO MemoryStore: MemoryStore cleared
18/06/29 03:51:39 INFO BlockManager: BlockManager stopped
18/06/29 03:51:39 INFO BlockManagerMaster: BlockManagerMaster stopped
18/06/29 03:51:39 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
18/06/29 03:51:39 WARN SparkEnv: Exception while deleting Spark temp dir: C:\Users\tehno-gate\AppData\Local\Temp\spark-415ebe47-7156-4ff7-9806-4da5e1159b5e\userFiles-8c0061d7-2923-4799-890b-5542fc4b02a9
java.io.IOException: Failed to delete: C:\Users\tehno-gate\AppData\Local\Temp\spark-415ebe47-7156-4ff7-9806-4da5e1159b5e\userFiles-8c0061d7-2923-4799-890b-5542fc4b02a9
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1031)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:103)
	at org.apache.spark.SparkContext$$anonfun$stop$11.apply$mcV$sp(SparkContext.scala:1937)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1936)
	at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:581)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1954)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
18/06/29 03:51:39 INFO SparkContext: Successfully stopped SparkContext
18/06/29 03:51:39 INFO ShutdownHookManager: Shutdown hook called
18/06/29 03:51:39 INFO ShutdownHookManager: Deleting directory C:\Users\tehno-gate\AppData\Local\Temp\spark-415ebe47-7156-4ff7-9806-4da5e1159b5e\userFiles-8c0061d7-2923-4799-890b-5542fc4b02a9
18/06/29 03:51:39 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\tehno-gate\AppData\Local\Temp\spark-415ebe47-7156-4ff7-9806-4da5e1159b5e\userFiles-8c0061d7-2923-4799-890b-5542fc4b02a9
java.io.IOException: Failed to delete: C:\Users\tehno-gate\AppData\Local\Temp\spark-415ebe47-7156-4ff7-9806-4da5e1159b5e\userFiles-8c0061d7-2923-4799-890b-5542fc4b02a9
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1031)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1954)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
18/06/29 03:51:39 INFO ShutdownHookManager: Deleting directory C:\Users\tehno-gate\AppData\Local\Temp\spark-415ebe47-7156-4ff7-9806-4da5e1159b5e
18/06/29 03:51:39 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\tehno-gate\AppData\Local\Temp\spark-415ebe47-7156-4ff7-9806-4da5e1159b5e
java.io.IOException: Failed to delete: C:\Users\tehno-gate\AppData\Local\Temp\spark-415ebe47-7156-4ff7-9806-4da5e1159b5e
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1031)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1954)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
